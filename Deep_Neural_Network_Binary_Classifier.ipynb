{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Neural Network Binary Classifier",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KieranJP/Neural-Network-Binary-Classifier/blob/master/Deep_Neural_Network_Binary_Classifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "taMS2EsFt6hm",
        "colab_type": "text"
      },
      "source": [
        "**This allows me to upload the csv files used as the training and the test data fom my PC:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s55CvUy25TWe",
        "colab_type": "code",
        "outputId": "df129ff5-cec7-47e8-d40d-128ee4435337",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "#Uploading Test Data\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e23f22ff-c0c4-4c83-bd90-cab2883f5f28\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e23f22ff-c0c4-4c83-bd90-cab2883f5f28\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving FullELFFDataset.csv to FullELFFDataset.csv\n",
            "User uploaded file \"FullELFFDataset.csv\" with length 14744861 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8BZisTk6HPl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import keras as K\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import pyplot\n",
        "\n",
        "#Imports Required\n",
        "import numpy\n",
        "import pandas\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "#Selecting the Csv to use then Displaying it\n",
        "import pandas as pd\n",
        "dataset = pd.read_csv(\"FullELFFDataset.csv\")\n",
        "\n",
        "#Using sklearn's split function to split the data into training and testing data.\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = dataset.values #drop defectives\n",
        "Y = dataset.Defective.values\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y,test_size=0.3)\n",
        "\n",
        "X_gridtrain, X_gridtest, Y_gridtrain, Y_gridtest = train_test_split(X_train,Y_train, test_size=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8FEI3TuRC0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "\n",
        "#Attempt at creating a Grid Search to optimise hyperparamter\n",
        "\n",
        "def create_model(batch=64, epoch=500, optimizer='adam', learn_rate=0.01, activation='relu'):\n",
        "  model = Sequential()\n",
        "  model.add(K.layers.Dense(units=40, input_dim=40, activation='relu', kernel_initializer='truncated_normal')) \n",
        "  model.add(K.layers.Dense(units=32, activation='relu', kernel_initializer='truncated_normal')) \n",
        "  model.add(K.layers.Dense(units=32, activation='relu', kernel_initializer='truncated_normal'))\n",
        "  model.add(K.layers.Dense(units=1, activation='sigmoid', kernel_initializer='truncated_normal'))\n",
        "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy']) \n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnhCoMHUYpni",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparamsBatch = [64, 128]\n",
        "hparamsEpoch = [250, 500, 1000]\n",
        "hparamsLR = [0.000001, 0.00001, 0.0001, 0.001, 0.01]\n",
        "\n",
        "# create model\n",
        "model = KerasClassifier(build_fn=create_model, verbose=0)\n",
        "\n",
        "#Defining the Range\n",
        "param_grid = dict(batch_size=hparamsBatch,\n",
        "                  epochs=hparamsEpoch,\n",
        "                  learn_rate=hparamsLR)\n",
        "\n",
        "grid = GridSearchCV(estimator=model, \n",
        "                    param_grid=param_grid,\n",
        "                    n_jobs=-1,\n",
        "                    cv=3,\n",
        "                    scoring='accuracy',\n",
        "                    verbose=10)\n",
        "grid_result = grid.fit(X_gridtest, Y_gridtest)\n",
        "\n",
        "# Show the results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOiTuEbyUMW9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Creates a NN\n",
        "model = Sequential()\n",
        "#Input Layer\n",
        "model.add(K.layers.Dense(units=40, input_dim=40, activation='relu', kernel_initializer='truncated_normal')) \n",
        "#Hidden Layers\n",
        "model.add(K.layers.Dense(units=32, activation='relu', kernel_initializer='truncated_normal')) \n",
        "model.add(K.layers.Dense(units=32, activation='relu', kernel_initializer='truncated_normal'))\n",
        "#Output Layers\n",
        "model.add(K.layers.Dense(units=1, activation='sigmoid', kernel_initializer='truncated_normal'))\n",
        "\n",
        "simple_sgd = K.optimizers.Adam(lr=0.00001)  \n",
        "model.compile(loss='binary_crossentropy', optimizer=simple_sgd, metrics=['accuracy']) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA4leyXvYfp1",
        "colab_type": "code",
        "outputId": "a73d7d79-830e-4ae2-d7a4-23004b6765ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34034
        }
      },
      "source": [
        "#Find out the size of each class so can change weighting\n",
        "trueWeight = len(Y_train[Y_train==False])\n",
        "falseWeight = len(Y_train[Y_train==True])\n",
        "\n",
        "history = model.fit(X_train, Y_train, batch_size=128, epochs=1000, validation_split=0.2)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 54924 samples, validate on 13732 samples\n",
            "Epoch 1/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.6977 - acc: 0.9502 - val_loss: 0.6044 - val_acc: 0.9498\n",
            "Epoch 2/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.5695 - acc: 0.9488 - val_loss: 0.5230 - val_acc: 0.9498\n",
            "Epoch 3/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.4534 - acc: 0.9489 - val_loss: 0.3908 - val_acc: 0.9498\n",
            "Epoch 4/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.3450 - acc: 0.9489 - val_loss: 0.3154 - val_acc: 0.9496\n",
            "Epoch 5/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.2978 - acc: 0.9491 - val_loss: 0.2844 - val_acc: 0.9496\n",
            "Epoch 6/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.2734 - acc: 0.9472 - val_loss: 0.2637 - val_acc: 0.9496\n",
            "Epoch 7/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.2545 - acc: 0.9492 - val_loss: 0.2466 - val_acc: 0.9494\n",
            "Epoch 8/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.2465 - acc: 0.9483 - val_loss: 0.2566 - val_acc: 0.9498\n",
            "Epoch 9/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.2383 - acc: 0.9495 - val_loss: 0.2294 - val_acc: 0.9490\n",
            "Epoch 10/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.2289 - acc: 0.9494 - val_loss: 0.2234 - val_acc: 0.9500\n",
            "Epoch 11/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.2232 - acc: 0.9491 - val_loss: 0.2652 - val_acc: 0.9274\n",
            "Epoch 12/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.2234 - acc: 0.9488 - val_loss: 0.2228 - val_acc: 0.9498\n",
            "Epoch 13/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.2182 - acc: 0.9492 - val_loss: 0.2130 - val_acc: 0.9494\n",
            "Epoch 14/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.2145 - acc: 0.9489 - val_loss: 0.2132 - val_acc: 0.9498\n",
            "Epoch 15/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.2061 - acc: 0.9497 - val_loss: 0.2124 - val_acc: 0.9498\n",
            "Epoch 16/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.2063 - acc: 0.9491 - val_loss: 0.2087 - val_acc: 0.9500\n",
            "Epoch 17/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.2102 - acc: 0.9485 - val_loss: 0.2130 - val_acc: 0.9442\n",
            "Epoch 18/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.2022 - acc: 0.9494 - val_loss: 0.2067 - val_acc: 0.9497\n",
            "Epoch 19/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.2040 - acc: 0.9488 - val_loss: 0.1982 - val_acc: 0.9482\n",
            "Epoch 20/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1992 - acc: 0.9492 - val_loss: 0.1984 - val_acc: 0.9482\n",
            "Epoch 21/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.2146 - acc: 0.9492 - val_loss: 0.1989 - val_acc: 0.9499\n",
            "Epoch 22/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1941 - acc: 0.9498 - val_loss: 0.1937 - val_acc: 0.9494\n",
            "Epoch 23/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.2013 - acc: 0.9491 - val_loss: 0.2056 - val_acc: 0.9499\n",
            "Epoch 24/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1934 - acc: 0.9493 - val_loss: 0.1989 - val_acc: 0.9501\n",
            "Epoch 25/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1924 - acc: 0.9494 - val_loss: 0.1945 - val_acc: 0.9498\n",
            "Epoch 26/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.1986 - acc: 0.9484 - val_loss: 0.2000 - val_acc: 0.9457\n",
            "Epoch 27/1000\n",
            "54924/54924 [==============================] - 1s 20us/step - loss: 0.1966 - acc: 0.9484 - val_loss: 0.1903 - val_acc: 0.9480\n",
            "Epoch 28/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.1918 - acc: 0.9493 - val_loss: 0.1983 - val_acc: 0.9498\n",
            "Epoch 29/1000\n",
            "54924/54924 [==============================] - 1s 20us/step - loss: 0.1937 - acc: 0.9485 - val_loss: 0.2565 - val_acc: 0.9302\n",
            "Epoch 30/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1960 - acc: 0.9485 - val_loss: 0.2098 - val_acc: 0.9498\n",
            "Epoch 31/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.2029 - acc: 0.9481 - val_loss: 0.2145 - val_acc: 0.9498\n",
            "Epoch 32/1000\n",
            "54924/54924 [==============================] - 1s 21us/step - loss: 0.1901 - acc: 0.9486 - val_loss: 0.1847 - val_acc: 0.9493\n",
            "Epoch 33/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1869 - acc: 0.9493 - val_loss: 0.2251 - val_acc: 0.9498\n",
            "Epoch 34/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1953 - acc: 0.9486 - val_loss: 0.2048 - val_acc: 0.9498\n",
            "Epoch 35/1000\n",
            "54924/54924 [==============================] - 1s 18us/step - loss: 0.1987 - acc: 0.9490 - val_loss: 0.1883 - val_acc: 0.9473\n",
            "Epoch 36/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1897 - acc: 0.9495 - val_loss: 0.1820 - val_acc: 0.9500\n",
            "Epoch 37/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1852 - acc: 0.9492 - val_loss: 0.1952 - val_acc: 0.9496\n",
            "Epoch 38/1000\n",
            "54924/54924 [==============================] - 1s 19us/step - loss: 0.1896 - acc: 0.9483 - val_loss: 0.1894 - val_acc: 0.9468\n",
            "Epoch 39/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1864 - acc: 0.9488 - val_loss: 0.2039 - val_acc: 0.9497\n",
            "Epoch 40/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1892 - acc: 0.9496 - val_loss: 0.1845 - val_acc: 0.9500\n",
            "Epoch 41/1000\n",
            "54924/54924 [==============================] - 1s 19us/step - loss: 0.1883 - acc: 0.9493 - val_loss: 0.1839 - val_acc: 0.9501\n",
            "Epoch 42/1000\n",
            "54924/54924 [==============================] - 1s 18us/step - loss: 0.1829 - acc: 0.9497 - val_loss: 0.2049 - val_acc: 0.9498\n",
            "Epoch 43/1000\n",
            "54924/54924 [==============================] - 1s 18us/step - loss: 0.1917 - acc: 0.9479 - val_loss: 0.1796 - val_acc: 0.9500\n",
            "Epoch 44/1000\n",
            "54924/54924 [==============================] - 1s 18us/step - loss: 0.1861 - acc: 0.9485 - val_loss: 0.1847 - val_acc: 0.9497\n",
            "Epoch 45/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1872 - acc: 0.9491 - val_loss: 0.1843 - val_acc: 0.9476\n",
            "Epoch 46/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1872 - acc: 0.9493 - val_loss: 0.1782 - val_acc: 0.9493\n",
            "Epoch 47/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1872 - acc: 0.9487 - val_loss: 0.1791 - val_acc: 0.9490\n",
            "Epoch 48/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1816 - acc: 0.9495 - val_loss: 0.1792 - val_acc: 0.9501\n",
            "Epoch 49/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1817 - acc: 0.9499 - val_loss: 0.1798 - val_acc: 0.9491\n",
            "Epoch 50/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1825 - acc: 0.9493 - val_loss: 0.1936 - val_acc: 0.9498\n",
            "Epoch 51/1000\n",
            "54924/54924 [==============================] - 1s 18us/step - loss: 0.1817 - acc: 0.9490 - val_loss: 0.1967 - val_acc: 0.9441\n",
            "Epoch 52/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1820 - acc: 0.9485 - val_loss: 0.1993 - val_acc: 0.9498\n",
            "Epoch 53/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1849 - acc: 0.9493 - val_loss: 0.1958 - val_acc: 0.9498\n",
            "Epoch 54/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1842 - acc: 0.9487 - val_loss: 0.1784 - val_acc: 0.9500\n",
            "Epoch 55/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1805 - acc: 0.9495 - val_loss: 0.1818 - val_acc: 0.9498\n",
            "Epoch 56/1000\n",
            "54924/54924 [==============================] - 1s 17us/step - loss: 0.1844 - acc: 0.9487 - val_loss: 0.2147 - val_acc: 0.9498\n",
            "Epoch 57/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1843 - acc: 0.9492 - val_loss: 0.1804 - val_acc: 0.9496\n",
            "Epoch 58/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1772 - acc: 0.9500 - val_loss: 0.1809 - val_acc: 0.9498\n",
            "Epoch 59/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1994 - acc: 0.9495 - val_loss: 0.1843 - val_acc: 0.9494\n",
            "Epoch 60/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1780 - acc: 0.9500 - val_loss: 0.1750 - val_acc: 0.9500\n",
            "Epoch 61/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1760 - acc: 0.9501 - val_loss: 0.1791 - val_acc: 0.9500\n",
            "Epoch 62/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1787 - acc: 0.9493 - val_loss: 0.1986 - val_acc: 0.9498\n",
            "Epoch 63/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1785 - acc: 0.9491 - val_loss: 0.2062 - val_acc: 0.9497\n",
            "Epoch 64/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1830 - acc: 0.9491 - val_loss: 0.1771 - val_acc: 0.9503\n",
            "Epoch 65/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1790 - acc: 0.9501 - val_loss: 0.1780 - val_acc: 0.9500\n",
            "Epoch 66/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.2012 - acc: 0.9500 - val_loss: 0.1773 - val_acc: 0.9498\n",
            "Epoch 67/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1820 - acc: 0.9491 - val_loss: 0.1799 - val_acc: 0.9494\n",
            "Epoch 68/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1802 - acc: 0.9496 - val_loss: 0.2127 - val_acc: 0.9498\n",
            "Epoch 69/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1825 - acc: 0.9493 - val_loss: 0.1803 - val_acc: 0.9478\n",
            "Epoch 70/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1804 - acc: 0.9496 - val_loss: 0.1776 - val_acc: 0.9498\n",
            "Epoch 71/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1803 - acc: 0.9497 - val_loss: 0.1772 - val_acc: 0.9500\n",
            "Epoch 72/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1746 - acc: 0.9500 - val_loss: 0.1751 - val_acc: 0.9497\n",
            "Epoch 73/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1823 - acc: 0.9492 - val_loss: 0.1967 - val_acc: 0.9465\n",
            "Epoch 74/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1833 - acc: 0.9496 - val_loss: 0.1769 - val_acc: 0.9497\n",
            "Epoch 75/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1804 - acc: 0.9500 - val_loss: 0.2190 - val_acc: 0.9382\n",
            "Epoch 76/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1768 - acc: 0.9494 - val_loss: 0.1768 - val_acc: 0.9498\n",
            "Epoch 77/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1852 - acc: 0.9489 - val_loss: 0.1989 - val_acc: 0.9498\n",
            "Epoch 78/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1798 - acc: 0.9495 - val_loss: 0.1878 - val_acc: 0.9498\n",
            "Epoch 79/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1836 - acc: 0.9497 - val_loss: 0.1845 - val_acc: 0.9498\n",
            "Epoch 80/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1802 - acc: 0.9495 - val_loss: 0.2019 - val_acc: 0.9498\n",
            "Epoch 81/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1847 - acc: 0.9499 - val_loss: 0.1802 - val_acc: 0.9500\n",
            "Epoch 82/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1779 - acc: 0.9500 - val_loss: 0.2096 - val_acc: 0.9498\n",
            "Epoch 83/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1841 - acc: 0.9494 - val_loss: 0.1756 - val_acc: 0.9498\n",
            "Epoch 84/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1816 - acc: 0.9495 - val_loss: 0.1811 - val_acc: 0.9505\n",
            "Epoch 85/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1771 - acc: 0.9499 - val_loss: 0.1763 - val_acc: 0.9498\n",
            "Epoch 86/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1774 - acc: 0.9499 - val_loss: 0.1826 - val_acc: 0.9474\n",
            "Epoch 87/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1824 - acc: 0.9495 - val_loss: 0.1784 - val_acc: 0.9498\n",
            "Epoch 88/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1847 - acc: 0.9486 - val_loss: 0.1724 - val_acc: 0.9495\n",
            "Epoch 89/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1759 - acc: 0.9497 - val_loss: 0.1779 - val_acc: 0.9500\n",
            "Epoch 90/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1870 - acc: 0.9494 - val_loss: 0.1738 - val_acc: 0.9503\n",
            "Epoch 91/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1792 - acc: 0.9493 - val_loss: 0.1812 - val_acc: 0.9500\n",
            "Epoch 92/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1815 - acc: 0.9491 - val_loss: 0.1778 - val_acc: 0.9503\n",
            "Epoch 93/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1756 - acc: 0.9499 - val_loss: 0.1770 - val_acc: 0.9498\n",
            "Epoch 94/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1789 - acc: 0.9490 - val_loss: 0.1805 - val_acc: 0.9498\n",
            "Epoch 95/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1778 - acc: 0.9499 - val_loss: 0.1707 - val_acc: 0.9498\n",
            "Epoch 96/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1808 - acc: 0.9492 - val_loss: 0.1699 - val_acc: 0.9503\n",
            "Epoch 97/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1795 - acc: 0.9492 - val_loss: 0.1783 - val_acc: 0.9500\n",
            "Epoch 98/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1776 - acc: 0.9497 - val_loss: 0.1823 - val_acc: 0.9498\n",
            "Epoch 99/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1770 - acc: 0.9500 - val_loss: 0.1717 - val_acc: 0.9498\n",
            "Epoch 100/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1787 - acc: 0.9493 - val_loss: 0.1740 - val_acc: 0.9500\n",
            "Epoch 101/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1814 - acc: 0.9489 - val_loss: 0.1827 - val_acc: 0.9502\n",
            "Epoch 102/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1747 - acc: 0.9500 - val_loss: 0.1876 - val_acc: 0.9498\n",
            "Epoch 103/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1728 - acc: 0.9500 - val_loss: 0.1767 - val_acc: 0.9500\n",
            "Epoch 104/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1790 - acc: 0.9491 - val_loss: 0.1723 - val_acc: 0.9500\n",
            "Epoch 105/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1784 - acc: 0.9495 - val_loss: 0.1729 - val_acc: 0.9500\n",
            "Epoch 106/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1757 - acc: 0.9495 - val_loss: 0.1756 - val_acc: 0.9482\n",
            "Epoch 107/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1762 - acc: 0.9493 - val_loss: 0.1805 - val_acc: 0.9499\n",
            "Epoch 108/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1759 - acc: 0.9496 - val_loss: 0.1715 - val_acc: 0.9500\n",
            "Epoch 109/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1794 - acc: 0.9493 - val_loss: 0.1740 - val_acc: 0.9486\n",
            "Epoch 110/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1747 - acc: 0.9496 - val_loss: 0.1902 - val_acc: 0.9446\n",
            "Epoch 111/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1765 - acc: 0.9495 - val_loss: 0.1778 - val_acc: 0.9500\n",
            "Epoch 112/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1776 - acc: 0.9493 - val_loss: 0.1754 - val_acc: 0.9498\n",
            "Epoch 113/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1757 - acc: 0.9496 - val_loss: 0.1737 - val_acc: 0.9495\n",
            "Epoch 114/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1730 - acc: 0.9496 - val_loss: 0.1690 - val_acc: 0.9505\n",
            "Epoch 115/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1746 - acc: 0.9495 - val_loss: 0.1703 - val_acc: 0.9504\n",
            "Epoch 116/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1747 - acc: 0.9497 - val_loss: 0.1707 - val_acc: 0.9500\n",
            "Epoch 117/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1733 - acc: 0.9497 - val_loss: 0.1785 - val_acc: 0.9483\n",
            "Epoch 118/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1729 - acc: 0.9503 - val_loss: 0.1682 - val_acc: 0.9504\n",
            "Epoch 119/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1752 - acc: 0.9496 - val_loss: 0.2151 - val_acc: 0.9364\n",
            "Epoch 120/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1767 - acc: 0.9494 - val_loss: 0.1715 - val_acc: 0.9505\n",
            "Epoch 121/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1724 - acc: 0.9499 - val_loss: 0.1785 - val_acc: 0.9500\n",
            "Epoch 122/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1809 - acc: 0.9490 - val_loss: 0.1907 - val_acc: 0.9498\n",
            "Epoch 123/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1710 - acc: 0.9501 - val_loss: 0.1839 - val_acc: 0.9498\n",
            "Epoch 124/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1772 - acc: 0.9492 - val_loss: 0.1759 - val_acc: 0.9498\n",
            "Epoch 125/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1728 - acc: 0.9499 - val_loss: 0.1736 - val_acc: 0.9501\n",
            "Epoch 126/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1720 - acc: 0.9498 - val_loss: 0.1701 - val_acc: 0.9499\n",
            "Epoch 127/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1727 - acc: 0.9501 - val_loss: 0.1703 - val_acc: 0.9503\n",
            "Epoch 128/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1737 - acc: 0.9501 - val_loss: 0.1731 - val_acc: 0.9487\n",
            "Epoch 129/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1705 - acc: 0.9501 - val_loss: 0.1763 - val_acc: 0.9482\n",
            "Epoch 130/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1785 - acc: 0.9494 - val_loss: 0.1716 - val_acc: 0.9504\n",
            "Epoch 131/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1730 - acc: 0.9504 - val_loss: 0.1733 - val_acc: 0.9501\n",
            "Epoch 132/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1691 - acc: 0.9501 - val_loss: 0.1665 - val_acc: 0.9503\n",
            "Epoch 133/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1702 - acc: 0.9504 - val_loss: 0.1700 - val_acc: 0.9503\n",
            "Epoch 134/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1709 - acc: 0.9500 - val_loss: 0.1842 - val_acc: 0.9498\n",
            "Epoch 135/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1779 - acc: 0.9496 - val_loss: 0.1676 - val_acc: 0.9498\n",
            "Epoch 136/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1717 - acc: 0.9499 - val_loss: 0.1715 - val_acc: 0.9498\n",
            "Epoch 137/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1825 - acc: 0.9498 - val_loss: 0.1725 - val_acc: 0.9502\n",
            "Epoch 138/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1726 - acc: 0.9499 - val_loss: 0.1707 - val_acc: 0.9503\n",
            "Epoch 139/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1661 - acc: 0.9503 - val_loss: 0.1658 - val_acc: 0.9503\n",
            "Epoch 140/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1751 - acc: 0.9494 - val_loss: 0.1689 - val_acc: 0.9502\n",
            "Epoch 141/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1714 - acc: 0.9498 - val_loss: 0.1651 - val_acc: 0.9505\n",
            "Epoch 142/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1762 - acc: 0.9492 - val_loss: 0.1800 - val_acc: 0.9497\n",
            "Epoch 143/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.1769 - acc: 0.9486 - val_loss: 0.1662 - val_acc: 0.9503\n",
            "Epoch 144/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1721 - acc: 0.9496 - val_loss: 0.2385 - val_acc: 0.9275\n",
            "Epoch 145/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1694 - acc: 0.9500 - val_loss: 0.1687 - val_acc: 0.9503\n",
            "Epoch 146/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1703 - acc: 0.9499 - val_loss: 0.1667 - val_acc: 0.9496\n",
            "Epoch 147/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1755 - acc: 0.9495 - val_loss: 0.1669 - val_acc: 0.9502\n",
            "Epoch 148/1000\n",
            "54924/54924 [==============================] - 1s 22us/step - loss: 0.1746 - acc: 0.9498 - val_loss: 0.1723 - val_acc: 0.9484\n",
            "Epoch 149/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1725 - acc: 0.9500 - val_loss: 0.1695 - val_acc: 0.9502\n",
            "Epoch 150/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1718 - acc: 0.9495 - val_loss: 0.1718 - val_acc: 0.9501\n",
            "Epoch 151/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1775 - acc: 0.9497 - val_loss: 0.1728 - val_acc: 0.9500\n",
            "Epoch 152/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1761 - acc: 0.9496 - val_loss: 0.1743 - val_acc: 0.9498\n",
            "Epoch 153/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1772 - acc: 0.9496 - val_loss: 0.1989 - val_acc: 0.9498\n",
            "Epoch 154/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1735 - acc: 0.9495 - val_loss: 0.1833 - val_acc: 0.9498\n",
            "Epoch 155/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.1711 - acc: 0.9501 - val_loss: 0.1636 - val_acc: 0.9500\n",
            "Epoch 156/1000\n",
            "54924/54924 [==============================] - 1s 23us/step - loss: 0.2396 - acc: 0.9495 - val_loss: 0.1941 - val_acc: 0.9454\n",
            "Epoch 157/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1774 - acc: 0.9493 - val_loss: 0.1960 - val_acc: 0.9492\n",
            "Epoch 158/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1707 - acc: 0.9495 - val_loss: 0.1682 - val_acc: 0.9487\n",
            "Epoch 159/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1684 - acc: 0.9499 - val_loss: 0.1828 - val_acc: 0.9500\n",
            "Epoch 160/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1724 - acc: 0.9498 - val_loss: 0.1747 - val_acc: 0.9473\n",
            "Epoch 161/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1691 - acc: 0.9496 - val_loss: 0.2015 - val_acc: 0.9498\n",
            "Epoch 162/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1686 - acc: 0.9501 - val_loss: 0.1716 - val_acc: 0.9494\n",
            "Epoch 163/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1677 - acc: 0.9501 - val_loss: 0.2009 - val_acc: 0.9498\n",
            "Epoch 164/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1703 - acc: 0.9503 - val_loss: 0.1680 - val_acc: 0.9502\n",
            "Epoch 165/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1730 - acc: 0.9491 - val_loss: 0.1803 - val_acc: 0.9495\n",
            "Epoch 166/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1751 - acc: 0.9497 - val_loss: 0.1791 - val_acc: 0.9497\n",
            "Epoch 167/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1708 - acc: 0.9497 - val_loss: 0.1662 - val_acc: 0.9495\n",
            "Epoch 168/1000\n",
            "54924/54924 [==============================] - 2s 27us/step - loss: 0.1673 - acc: 0.9501 - val_loss: 0.1667 - val_acc: 0.9500\n",
            "Epoch 169/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1754 - acc: 0.9501 - val_loss: 0.1901 - val_acc: 0.9497\n",
            "Epoch 170/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1705 - acc: 0.9499 - val_loss: 0.1682 - val_acc: 0.9500\n",
            "Epoch 171/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1706 - acc: 0.9495 - val_loss: 0.1642 - val_acc: 0.9503\n",
            "Epoch 172/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1851 - acc: 0.9496 - val_loss: 0.1633 - val_acc: 0.9503\n",
            "Epoch 173/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1712 - acc: 0.9502 - val_loss: 0.2075 - val_acc: 0.9391\n",
            "Epoch 174/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1687 - acc: 0.9502 - val_loss: 0.1736 - val_acc: 0.9498\n",
            "Epoch 175/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1709 - acc: 0.9497 - val_loss: 0.1824 - val_acc: 0.9452\n",
            "Epoch 176/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1714 - acc: 0.9495 - val_loss: 0.1711 - val_acc: 0.9501\n",
            "Epoch 177/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1699 - acc: 0.9497 - val_loss: 0.1633 - val_acc: 0.9508\n",
            "Epoch 178/1000\n",
            "54924/54924 [==============================] - 2s 27us/step - loss: 0.1713 - acc: 0.9496 - val_loss: 0.1795 - val_acc: 0.9498\n",
            "Epoch 179/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1735 - acc: 0.9493 - val_loss: 0.2008 - val_acc: 0.9498\n",
            "Epoch 180/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1682 - acc: 0.9503 - val_loss: 0.1763 - val_acc: 0.9498\n",
            "Epoch 181/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1661 - acc: 0.9499 - val_loss: 0.1762 - val_acc: 0.9500\n",
            "Epoch 182/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1678 - acc: 0.9505 - val_loss: 0.1676 - val_acc: 0.9495\n",
            "Epoch 183/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1703 - acc: 0.9498 - val_loss: 0.1955 - val_acc: 0.9415\n",
            "Epoch 184/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1706 - acc: 0.9494 - val_loss: 0.1715 - val_acc: 0.9498\n",
            "Epoch 185/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1722 - acc: 0.9495 - val_loss: 0.1606 - val_acc: 0.9508\n",
            "Epoch 186/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1698 - acc: 0.9503 - val_loss: 0.1634 - val_acc: 0.9506\n",
            "Epoch 187/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1641 - acc: 0.9505 - val_loss: 0.1639 - val_acc: 0.9501\n",
            "Epoch 188/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1682 - acc: 0.9498 - val_loss: 0.1608 - val_acc: 0.9506\n",
            "Epoch 189/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1734 - acc: 0.9491 - val_loss: 0.1641 - val_acc: 0.9501\n",
            "Epoch 190/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.2181 - acc: 0.9502 - val_loss: 0.1629 - val_acc: 0.9508\n",
            "Epoch 191/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1641 - acc: 0.9503 - val_loss: 0.1604 - val_acc: 0.9511\n",
            "Epoch 192/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1651 - acc: 0.9504 - val_loss: 0.1584 - val_acc: 0.9512\n",
            "Epoch 193/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1691 - acc: 0.9501 - val_loss: 0.1612 - val_acc: 0.9505\n",
            "Epoch 194/1000\n",
            "54924/54924 [==============================] - 2s 30us/step - loss: 0.1660 - acc: 0.9502 - val_loss: 0.1602 - val_acc: 0.9513\n",
            "Epoch 195/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1674 - acc: 0.9505 - val_loss: 0.1692 - val_acc: 0.9502\n",
            "Epoch 196/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1752 - acc: 0.9498 - val_loss: 0.1626 - val_acc: 0.9505\n",
            "Epoch 197/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1670 - acc: 0.9502 - val_loss: 0.1765 - val_acc: 0.9497\n",
            "Epoch 198/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1664 - acc: 0.9508 - val_loss: 0.1578 - val_acc: 0.9513\n",
            "Epoch 199/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1678 - acc: 0.9496 - val_loss: 0.1595 - val_acc: 0.9506\n",
            "Epoch 200/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1669 - acc: 0.9506 - val_loss: 0.1590 - val_acc: 0.9514\n",
            "Epoch 201/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1668 - acc: 0.9497 - val_loss: 0.1593 - val_acc: 0.9517\n",
            "Epoch 202/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1709 - acc: 0.9496 - val_loss: 0.1631 - val_acc: 0.9501\n",
            "Epoch 203/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1680 - acc: 0.9494 - val_loss: 0.1710 - val_acc: 0.9502\n",
            "Epoch 204/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1639 - acc: 0.9508 - val_loss: 0.1607 - val_acc: 0.9506\n",
            "Epoch 205/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1689 - acc: 0.9497 - val_loss: 0.2035 - val_acc: 0.9383\n",
            "Epoch 206/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1724 - acc: 0.9502 - val_loss: 0.1715 - val_acc: 0.9501\n",
            "Epoch 207/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1674 - acc: 0.9500 - val_loss: 0.1582 - val_acc: 0.9512\n",
            "Epoch 208/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1665 - acc: 0.9497 - val_loss: 0.1650 - val_acc: 0.9501\n",
            "Epoch 209/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1645 - acc: 0.9505 - val_loss: 0.1857 - val_acc: 0.9498\n",
            "Epoch 210/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1649 - acc: 0.9504 - val_loss: 0.1706 - val_acc: 0.9500\n",
            "Epoch 211/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1691 - acc: 0.9488 - val_loss: 0.1559 - val_acc: 0.9508\n",
            "Epoch 212/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1637 - acc: 0.9503 - val_loss: 0.1611 - val_acc: 0.9502\n",
            "Epoch 213/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1639 - acc: 0.9501 - val_loss: 0.1596 - val_acc: 0.9503\n",
            "Epoch 214/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1714 - acc: 0.9496 - val_loss: 0.1648 - val_acc: 0.9502\n",
            "Epoch 215/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1623 - acc: 0.9508 - val_loss: 0.1586 - val_acc: 0.9502\n",
            "Epoch 216/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1632 - acc: 0.9496 - val_loss: 0.1553 - val_acc: 0.9504\n",
            "Epoch 217/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1674 - acc: 0.9499 - val_loss: 0.1564 - val_acc: 0.9503\n",
            "Epoch 218/1000\n",
            "54924/54924 [==============================] - 2s 31us/step - loss: 0.1697 - acc: 0.9494 - val_loss: 0.1571 - val_acc: 0.9509\n",
            "Epoch 219/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1625 - acc: 0.9510 - val_loss: 0.1597 - val_acc: 0.9508\n",
            "Epoch 220/1000\n",
            "54924/54924 [==============================] - 2s 27us/step - loss: 0.1751 - acc: 0.9507 - val_loss: 0.1567 - val_acc: 0.9518\n",
            "Epoch 221/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1650 - acc: 0.9510 - val_loss: 0.1601 - val_acc: 0.9517\n",
            "Epoch 222/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1658 - acc: 0.9500 - val_loss: 0.2033 - val_acc: 0.9409\n",
            "Epoch 223/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1656 - acc: 0.9506 - val_loss: 0.1531 - val_acc: 0.9519\n",
            "Epoch 224/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1672 - acc: 0.9506 - val_loss: 0.1664 - val_acc: 0.9501\n",
            "Epoch 225/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1660 - acc: 0.9495 - val_loss: 0.1778 - val_acc: 0.9500\n",
            "Epoch 226/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1597 - acc: 0.9509 - val_loss: 0.1892 - val_acc: 0.9498\n",
            "Epoch 227/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1638 - acc: 0.9500 - val_loss: 0.1659 - val_acc: 0.9502\n",
            "Epoch 228/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1866 - acc: 0.9503 - val_loss: 0.1578 - val_acc: 0.9516\n",
            "Epoch 229/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1632 - acc: 0.9499 - val_loss: 0.1524 - val_acc: 0.9515\n",
            "Epoch 230/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1612 - acc: 0.9505 - val_loss: 0.1701 - val_acc: 0.9501\n",
            "Epoch 231/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1630 - acc: 0.9497 - val_loss: 0.1516 - val_acc: 0.9522\n",
            "Epoch 232/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1625 - acc: 0.9501 - val_loss: 0.1564 - val_acc: 0.9525\n",
            "Epoch 233/1000\n",
            "54924/54924 [==============================] - 2s 27us/step - loss: 0.1591 - acc: 0.9504 - val_loss: 0.1538 - val_acc: 0.9520\n",
            "Epoch 234/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1596 - acc: 0.9504 - val_loss: 0.1535 - val_acc: 0.9518\n",
            "Epoch 235/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1586 - acc: 0.9509 - val_loss: 0.2019 - val_acc: 0.9498\n",
            "Epoch 236/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1581 - acc: 0.9510 - val_loss: 0.1799 - val_acc: 0.9498\n",
            "Epoch 237/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1603 - acc: 0.9505 - val_loss: 0.2244 - val_acc: 0.9501\n",
            "Epoch 238/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1653 - acc: 0.9505 - val_loss: 0.1686 - val_acc: 0.9465\n",
            "Epoch 239/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1595 - acc: 0.9503 - val_loss: 0.1564 - val_acc: 0.9503\n",
            "Epoch 240/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1602 - acc: 0.9500 - val_loss: 0.1745 - val_acc: 0.9498\n",
            "Epoch 241/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1620 - acc: 0.9507 - val_loss: 0.1533 - val_acc: 0.9519\n",
            "Epoch 242/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.1581 - acc: 0.9501 - val_loss: 0.1528 - val_acc: 0.9522\n",
            "Epoch 243/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1631 - acc: 0.9500 - val_loss: 0.1485 - val_acc: 0.9523\n",
            "Epoch 244/1000\n",
            "54924/54924 [==============================] - 2s 30us/step - loss: 0.1613 - acc: 0.9497 - val_loss: 0.1552 - val_acc: 0.9509\n",
            "Epoch 245/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1599 - acc: 0.9505 - val_loss: 0.1578 - val_acc: 0.9525\n",
            "Epoch 246/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1583 - acc: 0.9507 - val_loss: 0.1951 - val_acc: 0.9382\n",
            "Epoch 247/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1643 - acc: 0.9494 - val_loss: 0.1494 - val_acc: 0.9522\n",
            "Epoch 248/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1546 - acc: 0.9510 - val_loss: 0.1492 - val_acc: 0.9524\n",
            "Epoch 249/1000\n",
            "54924/54924 [==============================] - 1s 24us/step - loss: 0.1636 - acc: 0.9513 - val_loss: 0.1889 - val_acc: 0.9503\n",
            "Epoch 250/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1655 - acc: 0.9504 - val_loss: 0.1639 - val_acc: 0.9511\n",
            "Epoch 251/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1555 - acc: 0.9512 - val_loss: 0.1665 - val_acc: 0.9501\n",
            "Epoch 252/1000\n",
            "54924/54924 [==============================] - 1s 25us/step - loss: 0.1590 - acc: 0.9510 - val_loss: 0.1565 - val_acc: 0.9516\n",
            "Epoch 253/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1552 - acc: 0.9509 - val_loss: 0.1497 - val_acc: 0.9514\n",
            "Epoch 254/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1574 - acc: 0.9509 - val_loss: 0.1757 - val_acc: 0.9501\n",
            "Epoch 255/1000\n",
            "54924/54924 [==============================] - 2s 28us/step - loss: 0.1536 - acc: 0.9506 - val_loss: 0.1612 - val_acc: 0.9487\n",
            "Epoch 256/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.1543 - acc: 0.9512 - val_loss: 0.1517 - val_acc: 0.9525\n",
            "Epoch 257/1000\n",
            "54924/54924 [==============================] - 1s 26us/step - loss: 0.1533 - acc: 0.9513 - val_loss: 0.1585 - val_acc: 0.9510\n",
            "Epoch 258/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.1571 - acc: 0.9497 - val_loss: 0.1500 - val_acc: 0.9525\n",
            "Epoch 259/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1533 - acc: 0.9510 - val_loss: 0.1553 - val_acc: 0.9510\n",
            "Epoch 260/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1508 - acc: 0.9516 - val_loss: 0.2016 - val_acc: 0.9498\n",
            "Epoch 261/1000\n",
            "54924/54924 [==============================] - 2s 32us/step - loss: 0.1548 - acc: 0.9519 - val_loss: 0.1471 - val_acc: 0.9527\n",
            "Epoch 262/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1543 - acc: 0.9508 - val_loss: 0.1453 - val_acc: 0.9527\n",
            "Epoch 263/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.1536 - acc: 0.9508 - val_loss: 0.1456 - val_acc: 0.9529\n",
            "Epoch 264/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.1532 - acc: 0.9511 - val_loss: 0.1628 - val_acc: 0.9506\n",
            "Epoch 265/1000\n",
            "54924/54924 [==============================] - 2s 36us/step - loss: 0.1629 - acc: 0.9502 - val_loss: 0.1506 - val_acc: 0.9516\n",
            "Epoch 266/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.1611 - acc: 0.9502 - val_loss: 0.1776 - val_acc: 0.9499\n",
            "Epoch 267/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1529 - acc: 0.9509 - val_loss: 0.1502 - val_acc: 0.9517\n",
            "Epoch 268/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1520 - acc: 0.9508 - val_loss: 0.1448 - val_acc: 0.9524\n",
            "Epoch 269/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1540 - acc: 0.9513 - val_loss: 0.1487 - val_acc: 0.9524\n",
            "Epoch 270/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.1535 - acc: 0.9510 - val_loss: 0.1454 - val_acc: 0.9530\n",
            "Epoch 271/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1502 - acc: 0.9511 - val_loss: 0.1979 - val_acc: 0.9504\n",
            "Epoch 272/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1527 - acc: 0.9503 - val_loss: 0.1486 - val_acc: 0.9512\n",
            "Epoch 273/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.1566 - acc: 0.9516 - val_loss: 0.1572 - val_acc: 0.9511\n",
            "Epoch 274/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1540 - acc: 0.9508 - val_loss: 0.1419 - val_acc: 0.9531\n",
            "Epoch 275/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1543 - acc: 0.9505 - val_loss: 0.1412 - val_acc: 0.9524\n",
            "Epoch 276/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1515 - acc: 0.9504 - val_loss: 0.1575 - val_acc: 0.9507\n",
            "Epoch 277/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1496 - acc: 0.9514 - val_loss: 0.1451 - val_acc: 0.9530\n",
            "Epoch 278/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1490 - acc: 0.9507 - val_loss: 0.1515 - val_acc: 0.9519\n",
            "Epoch 279/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1438 - acc: 0.9518 - val_loss: 0.1383 - val_acc: 0.9535\n",
            "Epoch 280/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1671 - acc: 0.9516 - val_loss: 0.1416 - val_acc: 0.9529\n",
            "Epoch 281/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.1513 - acc: 0.9524 - val_loss: 0.1435 - val_acc: 0.9530\n",
            "Epoch 282/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1461 - acc: 0.9513 - val_loss: 0.1531 - val_acc: 0.9533\n",
            "Epoch 283/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1482 - acc: 0.9517 - val_loss: 0.1392 - val_acc: 0.9532\n",
            "Epoch 284/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1509 - acc: 0.9509 - val_loss: 0.1376 - val_acc: 0.9545\n",
            "Epoch 285/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1445 - acc: 0.9523 - val_loss: 0.1636 - val_acc: 0.9500\n",
            "Epoch 286/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1483 - acc: 0.9517 - val_loss: 0.1358 - val_acc: 0.9540\n",
            "Epoch 287/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1434 - acc: 0.9517 - val_loss: 0.1403 - val_acc: 0.9538\n",
            "Epoch 288/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1428 - acc: 0.9516 - val_loss: 0.1527 - val_acc: 0.9516\n",
            "Epoch 289/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1425 - acc: 0.9521 - val_loss: 0.1450 - val_acc: 0.9517\n",
            "Epoch 290/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1465 - acc: 0.9518 - val_loss: 0.1703 - val_acc: 0.9516\n",
            "Epoch 291/1000\n",
            "54924/54924 [==============================] - 3s 58us/step - loss: 0.1671 - acc: 0.9519 - val_loss: 0.1398 - val_acc: 0.9540\n",
            "Epoch 292/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.1439 - acc: 0.9518 - val_loss: 0.1586 - val_acc: 0.9518\n",
            "Epoch 293/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1397 - acc: 0.9524 - val_loss: 0.1388 - val_acc: 0.9524\n",
            "Epoch 294/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1454 - acc: 0.9516 - val_loss: 0.1418 - val_acc: 0.9522\n",
            "Epoch 295/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.1428 - acc: 0.9518 - val_loss: 0.1316 - val_acc: 0.9535\n",
            "Epoch 296/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.1427 - acc: 0.9527 - val_loss: 0.1485 - val_acc: 0.9528\n",
            "Epoch 297/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.1416 - acc: 0.9523 - val_loss: 0.1330 - val_acc: 0.9543\n",
            "Epoch 298/1000\n",
            "54924/54924 [==============================] - 3s 57us/step - loss: 0.1381 - acc: 0.9532 - val_loss: 0.1556 - val_acc: 0.9512\n",
            "Epoch 299/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.1422 - acc: 0.9527 - val_loss: 0.1488 - val_acc: 0.9520\n",
            "Epoch 300/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1383 - acc: 0.9525 - val_loss: 0.1521 - val_acc: 0.9487\n",
            "Epoch 301/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1399 - acc: 0.9524 - val_loss: 0.1346 - val_acc: 0.9554\n",
            "Epoch 302/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1403 - acc: 0.9516 - val_loss: 0.1418 - val_acc: 0.9524\n",
            "Epoch 303/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1391 - acc: 0.9528 - val_loss: 0.1554 - val_acc: 0.9457\n",
            "Epoch 304/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.1394 - acc: 0.9527 - val_loss: 0.1372 - val_acc: 0.9524\n",
            "Epoch 305/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1394 - acc: 0.9518 - val_loss: 0.1334 - val_acc: 0.9558\n",
            "Epoch 306/1000\n",
            "54924/54924 [==============================] - 3s 58us/step - loss: 0.1313 - acc: 0.9540 - val_loss: 0.1245 - val_acc: 0.9562\n",
            "Epoch 307/1000\n",
            "54924/54924 [==============================] - 3s 56us/step - loss: 0.1378 - acc: 0.9529 - val_loss: 0.1263 - val_acc: 0.9564\n",
            "Epoch 308/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1365 - acc: 0.9530 - val_loss: 0.1380 - val_acc: 0.9559\n",
            "Epoch 309/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1321 - acc: 0.9539 - val_loss: 0.1590 - val_acc: 0.9515\n",
            "Epoch 310/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.1350 - acc: 0.9530 - val_loss: 0.1406 - val_acc: 0.9523\n",
            "Epoch 311/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1401 - acc: 0.9545 - val_loss: 0.1375 - val_acc: 0.9546\n",
            "Epoch 312/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1445 - acc: 0.9530 - val_loss: 0.1297 - val_acc: 0.9559\n",
            "Epoch 313/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1382 - acc: 0.9541 - val_loss: 0.1652 - val_acc: 0.9524\n",
            "Epoch 314/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1416 - acc: 0.9542 - val_loss: 0.1703 - val_acc: 0.9519\n",
            "Epoch 315/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1371 - acc: 0.9541 - val_loss: 0.1490 - val_acc: 0.9539\n",
            "Epoch 316/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.1294 - acc: 0.9553 - val_loss: 0.1287 - val_acc: 0.9578\n",
            "Epoch 317/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.1323 - acc: 0.9554 - val_loss: 0.1305 - val_acc: 0.9554\n",
            "Epoch 318/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.1347 - acc: 0.9550 - val_loss: 0.1298 - val_acc: 0.9555\n",
            "Epoch 319/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1351 - acc: 0.9552 - val_loss: 0.1198 - val_acc: 0.9583\n",
            "Epoch 320/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1304 - acc: 0.9554 - val_loss: 0.1274 - val_acc: 0.9555\n",
            "Epoch 321/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1322 - acc: 0.9548 - val_loss: 0.1220 - val_acc: 0.9581\n",
            "Epoch 322/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1387 - acc: 0.9541 - val_loss: 0.1432 - val_acc: 0.9542\n",
            "Epoch 323/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.1342 - acc: 0.9549 - val_loss: 0.1234 - val_acc: 0.9562\n",
            "Epoch 324/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1296 - acc: 0.9561 - val_loss: 0.1251 - val_acc: 0.9580\n",
            "Epoch 325/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1315 - acc: 0.9562 - val_loss: 0.1191 - val_acc: 0.9561\n",
            "Epoch 326/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1284 - acc: 0.9562 - val_loss: 0.1152 - val_acc: 0.9589\n",
            "Epoch 327/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1372 - acc: 0.9553 - val_loss: 0.1288 - val_acc: 0.9573\n",
            "Epoch 328/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1339 - acc: 0.9561 - val_loss: 0.1509 - val_acc: 0.9532\n",
            "Epoch 329/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1236 - acc: 0.9558 - val_loss: 0.1104 - val_acc: 0.9588\n",
            "Epoch 330/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1278 - acc: 0.9563 - val_loss: 0.1260 - val_acc: 0.9557\n",
            "Epoch 331/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.1258 - acc: 0.9555 - val_loss: 0.1470 - val_acc: 0.9516\n",
            "Epoch 332/1000\n",
            "54924/54924 [==============================] - 3s 58us/step - loss: 0.1330 - acc: 0.9553 - val_loss: 0.1531 - val_acc: 0.9542\n",
            "Epoch 333/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.1352 - acc: 0.9568 - val_loss: 0.1252 - val_acc: 0.9586\n",
            "Epoch 334/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1281 - acc: 0.9560 - val_loss: 0.1282 - val_acc: 0.9562\n",
            "Epoch 335/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1253 - acc: 0.9557 - val_loss: 0.1106 - val_acc: 0.9588\n",
            "Epoch 336/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1257 - acc: 0.9572 - val_loss: 0.1460 - val_acc: 0.9557\n",
            "Epoch 337/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1235 - acc: 0.9570 - val_loss: 0.1258 - val_acc: 0.9562\n",
            "Epoch 338/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1229 - acc: 0.9569 - val_loss: 0.1342 - val_acc: 0.9557\n",
            "Epoch 339/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1310 - acc: 0.9569 - val_loss: 0.1113 - val_acc: 0.9594\n",
            "Epoch 340/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1248 - acc: 0.9568 - val_loss: 0.1171 - val_acc: 0.9565\n",
            "Epoch 341/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.1316 - acc: 0.9565 - val_loss: 0.1134 - val_acc: 0.9598\n",
            "Epoch 342/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1340 - acc: 0.9573 - val_loss: 0.1111 - val_acc: 0.9583\n",
            "Epoch 343/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1257 - acc: 0.9570 - val_loss: 0.1095 - val_acc: 0.9589\n",
            "Epoch 344/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1195 - acc: 0.9577 - val_loss: 0.1086 - val_acc: 0.9594\n",
            "Epoch 345/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1228 - acc: 0.9563 - val_loss: 0.1140 - val_acc: 0.9581\n",
            "Epoch 346/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.1098 - acc: 0.9580 - val_loss: 0.0976 - val_acc: 0.9616\n",
            "Epoch 347/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1102 - acc: 0.9592 - val_loss: 0.1021 - val_acc: 0.9621\n",
            "Epoch 348/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1158 - acc: 0.9589 - val_loss: 0.0987 - val_acc: 0.9604\n",
            "Epoch 349/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1136 - acc: 0.9596 - val_loss: 0.0988 - val_acc: 0.9624\n",
            "Epoch 350/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.1217 - acc: 0.9592 - val_loss: 0.1196 - val_acc: 0.9573\n",
            "Epoch 351/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1188 - acc: 0.9581 - val_loss: 0.1118 - val_acc: 0.9621\n",
            "Epoch 352/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1140 - acc: 0.9595 - val_loss: 0.1070 - val_acc: 0.9616\n",
            "Epoch 353/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1092 - acc: 0.9600 - val_loss: 0.0940 - val_acc: 0.9618\n",
            "Epoch 354/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1122 - acc: 0.9593 - val_loss: 0.1124 - val_acc: 0.9611\n",
            "Epoch 355/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1043 - acc: 0.9612 - val_loss: 0.1037 - val_acc: 0.9629\n",
            "Epoch 356/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1065 - acc: 0.9608 - val_loss: 0.1065 - val_acc: 0.9616\n",
            "Epoch 357/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1111 - acc: 0.9607 - val_loss: 0.0943 - val_acc: 0.9626\n",
            "Epoch 358/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1148 - acc: 0.9603 - val_loss: 0.1381 - val_acc: 0.9572\n",
            "Epoch 359/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1205 - acc: 0.9601 - val_loss: 0.1819 - val_acc: 0.9461\n",
            "Epoch 360/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1174 - acc: 0.9609 - val_loss: 0.1035 - val_acc: 0.9628\n",
            "Epoch 361/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1611 - acc: 0.9599 - val_loss: 0.0934 - val_acc: 0.9647\n",
            "Epoch 362/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1192 - acc: 0.9616 - val_loss: 0.1838 - val_acc: 0.9412\n",
            "Epoch 363/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1109 - acc: 0.9617 - val_loss: 0.1006 - val_acc: 0.9654\n",
            "Epoch 364/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0985 - acc: 0.9637 - val_loss: 0.1117 - val_acc: 0.9610\n",
            "Epoch 365/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1232 - acc: 0.9605 - val_loss: 0.0960 - val_acc: 0.9651\n",
            "Epoch 366/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.1185 - acc: 0.9622 - val_loss: 0.1095 - val_acc: 0.9634\n",
            "Epoch 367/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1358 - acc: 0.9597 - val_loss: 0.1005 - val_acc: 0.9648\n",
            "Epoch 368/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1270 - acc: 0.9605 - val_loss: 0.0917 - val_acc: 0.9650\n",
            "Epoch 369/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0978 - acc: 0.9645 - val_loss: 0.0853 - val_acc: 0.9651\n",
            "Epoch 370/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1098 - acc: 0.9639 - val_loss: 0.0840 - val_acc: 0.9684\n",
            "Epoch 371/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0973 - acc: 0.9651 - val_loss: 0.0836 - val_acc: 0.9685\n",
            "Epoch 372/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0921 - acc: 0.9654 - val_loss: 0.0849 - val_acc: 0.9665\n",
            "Epoch 373/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1028 - acc: 0.9651 - val_loss: 0.0954 - val_acc: 0.9663\n",
            "Epoch 374/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1019 - acc: 0.9654 - val_loss: 0.1086 - val_acc: 0.9660\n",
            "Epoch 375/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0944 - acc: 0.9651 - val_loss: 0.1386 - val_acc: 0.9568\n",
            "Epoch 376/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0950 - acc: 0.9658 - val_loss: 0.0937 - val_acc: 0.9647\n",
            "Epoch 377/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1136 - acc: 0.9619 - val_loss: 0.0982 - val_acc: 0.9696\n",
            "Epoch 378/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0957 - acc: 0.9655 - val_loss: 0.1038 - val_acc: 0.9650\n",
            "Epoch 379/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.1302 - acc: 0.9632 - val_loss: 0.1296 - val_acc: 0.9645\n",
            "Epoch 380/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0887 - acc: 0.9679 - val_loss: 0.1217 - val_acc: 0.9628\n",
            "Epoch 381/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0958 - acc: 0.9666 - val_loss: 0.1052 - val_acc: 0.9656\n",
            "Epoch 382/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0876 - acc: 0.9676 - val_loss: 0.0971 - val_acc: 0.9703\n",
            "Epoch 383/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0959 - acc: 0.9672 - val_loss: 0.0780 - val_acc: 0.9739\n",
            "Epoch 384/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0872 - acc: 0.9686 - val_loss: 0.1171 - val_acc: 0.9645\n",
            "Epoch 385/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1122 - acc: 0.9659 - val_loss: 0.1037 - val_acc: 0.9648\n",
            "Epoch 386/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0944 - acc: 0.9673 - val_loss: 0.1360 - val_acc: 0.9613\n",
            "Epoch 387/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0950 - acc: 0.9678 - val_loss: 0.0801 - val_acc: 0.9710\n",
            "Epoch 388/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1023 - acc: 0.9667 - val_loss: 0.0917 - val_acc: 0.9656\n",
            "Epoch 389/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1061 - acc: 0.9675 - val_loss: 0.0809 - val_acc: 0.9716\n",
            "Epoch 390/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0862 - acc: 0.9696 - val_loss: 0.0771 - val_acc: 0.9725\n",
            "Epoch 391/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0803 - acc: 0.9716 - val_loss: 0.1711 - val_acc: 0.9452\n",
            "Epoch 392/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1008 - acc: 0.9709 - val_loss: 0.3319 - val_acc: 0.9565\n",
            "Epoch 393/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1027 - acc: 0.9702 - val_loss: 0.0734 - val_acc: 0.9711\n",
            "Epoch 394/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0783 - acc: 0.9721 - val_loss: 0.0800 - val_acc: 0.9731\n",
            "Epoch 395/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0788 - acc: 0.9727 - val_loss: 0.0717 - val_acc: 0.9822\n",
            "Epoch 396/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0863 - acc: 0.9719 - val_loss: 0.0632 - val_acc: 0.9814\n",
            "Epoch 397/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0822 - acc: 0.9735 - val_loss: 0.0825 - val_acc: 0.9835\n",
            "Epoch 398/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0726 - acc: 0.9745 - val_loss: 0.0657 - val_acc: 0.9744\n",
            "Epoch 399/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0850 - acc: 0.9738 - val_loss: 0.0719 - val_acc: 0.9808\n",
            "Epoch 400/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0680 - acc: 0.9757 - val_loss: 0.0619 - val_acc: 0.9773\n",
            "Epoch 401/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0851 - acc: 0.9743 - val_loss: 0.0608 - val_acc: 0.9787\n",
            "Epoch 402/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0846 - acc: 0.9723 - val_loss: 0.0763 - val_acc: 0.9752\n",
            "Epoch 403/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1117 - acc: 0.9739 - val_loss: 0.1115 - val_acc: 0.9699\n",
            "Epoch 404/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0698 - acc: 0.9759 - val_loss: 0.0962 - val_acc: 0.9731\n",
            "Epoch 405/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0829 - acc: 0.9761 - val_loss: 0.0646 - val_acc: 0.9792\n",
            "Epoch 406/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0880 - acc: 0.9760 - val_loss: 0.0551 - val_acc: 0.9826\n",
            "Epoch 407/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0641 - acc: 0.9789 - val_loss: 0.1422 - val_acc: 0.9629\n",
            "Epoch 408/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0849 - acc: 0.9773 - val_loss: 0.0578 - val_acc: 0.9826\n",
            "Epoch 409/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0632 - acc: 0.9797 - val_loss: 0.0686 - val_acc: 0.9844\n",
            "Epoch 410/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0902 - acc: 0.9768 - val_loss: 0.0571 - val_acc: 0.9868\n",
            "Epoch 411/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1770 - acc: 0.9746 - val_loss: 0.0559 - val_acc: 0.9811\n",
            "Epoch 412/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0674 - acc: 0.9795 - val_loss: 0.0961 - val_acc: 0.9736\n",
            "Epoch 413/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0703 - acc: 0.9803 - val_loss: 0.0609 - val_acc: 0.9862\n",
            "Epoch 414/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0679 - acc: 0.9816 - val_loss: 0.0557 - val_acc: 0.9870\n",
            "Epoch 415/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0698 - acc: 0.9813 - val_loss: 0.0745 - val_acc: 0.9776\n",
            "Epoch 416/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1104 - acc: 0.9748 - val_loss: 0.0543 - val_acc: 0.9839\n",
            "Epoch 417/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.0934 - acc: 0.9776 - val_loss: 0.1008 - val_acc: 0.9804\n",
            "Epoch 418/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0800 - acc: 0.9798 - val_loss: 0.0518 - val_acc: 0.9875\n",
            "Epoch 419/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0628 - acc: 0.9831 - val_loss: 0.0609 - val_acc: 0.9827\n",
            "Epoch 420/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.0748 - acc: 0.9803 - val_loss: 0.0973 - val_acc: 0.9739\n",
            "Epoch 421/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0803 - acc: 0.9809 - val_loss: 0.1281 - val_acc: 0.9653\n",
            "Epoch 422/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0787 - acc: 0.9805 - val_loss: 0.0518 - val_acc: 0.9845\n",
            "Epoch 423/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0899 - acc: 0.9810 - val_loss: 0.1232 - val_acc: 0.9703\n",
            "Epoch 424/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.2152 - acc: 0.9721 - val_loss: 0.0744 - val_acc: 0.9752\n",
            "Epoch 425/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0831 - acc: 0.9805 - val_loss: 0.0556 - val_acc: 0.9832\n",
            "Epoch 426/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0792 - acc: 0.9819 - val_loss: 0.0869 - val_acc: 0.9859\n",
            "Epoch 427/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0700 - acc: 0.9836 - val_loss: 0.0520 - val_acc: 0.9807\n",
            "Epoch 428/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0692 - acc: 0.9834 - val_loss: 0.0555 - val_acc: 0.9917\n",
            "Epoch 429/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0757 - acc: 0.9818 - val_loss: 0.1118 - val_acc: 0.9739\n",
            "Epoch 430/1000\n",
            "54924/54924 [==============================] - 2s 38us/step - loss: 0.0686 - acc: 0.9833 - val_loss: 0.0489 - val_acc: 0.9887\n",
            "Epoch 431/1000\n",
            "54924/54924 [==============================] - 2s 39us/step - loss: 0.0691 - acc: 0.9833 - val_loss: 0.1016 - val_acc: 0.9680\n",
            "Epoch 432/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0720 - acc: 0.9844 - val_loss: 0.0605 - val_acc: 0.9915\n",
            "Epoch 433/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0650 - acc: 0.9863 - val_loss: 0.1049 - val_acc: 0.9779\n",
            "Epoch 434/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0860 - acc: 0.9828 - val_loss: 0.1050 - val_acc: 0.9765\n",
            "Epoch 435/1000\n",
            "54924/54924 [==============================] - 2s 37us/step - loss: 0.0645 - acc: 0.9867 - val_loss: 0.0556 - val_acc: 0.9905\n",
            "Epoch 436/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0675 - acc: 0.9854 - val_loss: 0.0487 - val_acc: 0.9911\n",
            "Epoch 437/1000\n",
            "54924/54924 [==============================] - 2s 33us/step - loss: 0.0834 - acc: 0.9847 - val_loss: 0.1000 - val_acc: 0.9793\n",
            "Epoch 438/1000\n",
            "54924/54924 [==============================] - 1s 27us/step - loss: 0.0901 - acc: 0.9840 - val_loss: 0.0641 - val_acc: 0.9835\n",
            "Epoch 439/1000\n",
            "54924/54924 [==============================] - 2s 29us/step - loss: 0.1071 - acc: 0.9800 - val_loss: 0.1504 - val_acc: 0.9707\n",
            "Epoch 440/1000\n",
            "54924/54924 [==============================] - 2s 35us/step - loss: 0.0682 - acc: 0.9852 - val_loss: 0.0461 - val_acc: 0.9929\n",
            "Epoch 441/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0983 - acc: 0.9794 - val_loss: 0.0696 - val_acc: 0.9841\n",
            "Epoch 442/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0531 - acc: 0.9873 - val_loss: 0.0414 - val_acc: 0.9928\n",
            "Epoch 443/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0921 - acc: 0.9807 - val_loss: 0.3124 - val_acc: 0.9175\n",
            "Epoch 444/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0888 - acc: 0.9827 - val_loss: 0.0544 - val_acc: 0.9853\n",
            "Epoch 445/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0745 - acc: 0.9836 - val_loss: 0.0605 - val_acc: 0.9841\n",
            "Epoch 446/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0692 - acc: 0.9847 - val_loss: 0.1041 - val_acc: 0.9768\n",
            "Epoch 447/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0570 - acc: 0.9869 - val_loss: 0.0649 - val_acc: 0.9875\n",
            "Epoch 448/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0648 - acc: 0.9857 - val_loss: 0.0713 - val_acc: 0.9858\n",
            "Epoch 449/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1122 - acc: 0.9832 - val_loss: 0.0600 - val_acc: 0.9876\n",
            "Epoch 450/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1650 - acc: 0.9796 - val_loss: 0.0631 - val_acc: 0.9842\n",
            "Epoch 451/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0813 - acc: 0.9844 - val_loss: 0.0671 - val_acc: 0.9850\n",
            "Epoch 452/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.0549 - acc: 0.9883 - val_loss: 0.0515 - val_acc: 0.9937\n",
            "Epoch 453/1000\n",
            "54924/54924 [==============================] - 3s 56us/step - loss: 0.0594 - acc: 0.9877 - val_loss: 0.1481 - val_acc: 0.9742\n",
            "Epoch 454/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0609 - acc: 0.9867 - val_loss: 0.0567 - val_acc: 0.9943\n",
            "Epoch 455/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0700 - acc: 0.9866 - val_loss: 0.0728 - val_acc: 0.9822\n",
            "Epoch 456/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0597 - acc: 0.9865 - val_loss: 0.0363 - val_acc: 0.9921\n",
            "Epoch 457/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0747 - acc: 0.9868 - val_loss: 0.0822 - val_acc: 0.9810\n",
            "Epoch 458/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0728 - acc: 0.9858 - val_loss: 0.1813 - val_acc: 0.9448\n",
            "Epoch 459/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0983 - acc: 0.9819 - val_loss: 0.0421 - val_acc: 0.9904\n",
            "Epoch 460/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0531 - acc: 0.9887 - val_loss: 0.0581 - val_acc: 0.9857\n",
            "Epoch 461/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0831 - acc: 0.9854 - val_loss: 0.0952 - val_acc: 0.9787\n",
            "Epoch 462/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0511 - acc: 0.9890 - val_loss: 0.1414 - val_acc: 0.9758\n",
            "Epoch 463/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0652 - acc: 0.9869 - val_loss: 0.0645 - val_acc: 0.9941\n",
            "Epoch 464/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0602 - acc: 0.9880 - val_loss: 0.3505 - val_acc: 0.9045\n",
            "Epoch 465/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0859 - acc: 0.9843 - val_loss: 0.2671 - val_acc: 0.9285\n",
            "Epoch 466/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0585 - acc: 0.9883 - val_loss: 0.0338 - val_acc: 0.9938\n",
            "Epoch 467/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0831 - acc: 0.9841 - val_loss: 0.0416 - val_acc: 0.9878\n",
            "Epoch 468/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0684 - acc: 0.9872 - val_loss: 0.0319 - val_acc: 0.9932\n",
            "Epoch 469/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0492 - acc: 0.9902 - val_loss: 0.0341 - val_acc: 0.9948\n",
            "Epoch 470/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0732 - acc: 0.9849 - val_loss: 0.0369 - val_acc: 0.9937\n",
            "Epoch 471/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0490 - acc: 0.9903 - val_loss: 0.0381 - val_acc: 0.9944\n",
            "Epoch 472/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0690 - acc: 0.9872 - val_loss: 0.0718 - val_acc: 0.9850\n",
            "Epoch 473/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0726 - acc: 0.9860 - val_loss: 0.0639 - val_acc: 0.9936\n",
            "Epoch 474/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0631 - acc: 0.9870 - val_loss: 0.0476 - val_acc: 0.9948\n",
            "Epoch 475/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0561 - acc: 0.9894 - val_loss: 0.0435 - val_acc: 0.9929\n",
            "Epoch 476/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0476 - acc: 0.9909 - val_loss: 0.0477 - val_acc: 0.9905\n",
            "Epoch 477/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0625 - acc: 0.9888 - val_loss: 0.1071 - val_acc: 0.9807\n",
            "Epoch 478/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0613 - acc: 0.9885 - val_loss: 0.0335 - val_acc: 0.9955\n",
            "Epoch 479/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0561 - acc: 0.9879 - val_loss: 0.1275 - val_acc: 0.9776\n",
            "Epoch 480/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0560 - acc: 0.9885 - val_loss: 0.0318 - val_acc: 0.9956\n",
            "Epoch 481/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0588 - acc: 0.9887 - val_loss: 0.0374 - val_acc: 0.9926\n",
            "Epoch 482/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0586 - acc: 0.9887 - val_loss: 0.0379 - val_acc: 0.9942\n",
            "Epoch 483/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0815 - acc: 0.9851 - val_loss: 0.1765 - val_acc: 0.9719\n",
            "Epoch 484/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0782 - acc: 0.9854 - val_loss: 0.0391 - val_acc: 0.9953\n",
            "Epoch 485/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0816 - acc: 0.9865 - val_loss: 0.0457 - val_acc: 0.9953\n",
            "Epoch 486/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0484 - acc: 0.9905 - val_loss: 0.0336 - val_acc: 0.9900\n",
            "Epoch 487/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0882 - acc: 0.9875 - val_loss: 0.0565 - val_acc: 0.9887\n",
            "Epoch 488/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0431 - acc: 0.9911 - val_loss: 0.0299 - val_acc: 0.9954\n",
            "Epoch 489/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0484 - acc: 0.9908 - val_loss: 0.0361 - val_acc: 0.9940\n",
            "Epoch 490/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0898 - acc: 0.9843 - val_loss: 0.0403 - val_acc: 0.9926\n",
            "Epoch 491/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0586 - acc: 0.9877 - val_loss: 0.0584 - val_acc: 0.9843\n",
            "Epoch 492/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0575 - acc: 0.9897 - val_loss: 0.0339 - val_acc: 0.9956\n",
            "Epoch 493/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0976 - acc: 0.9857 - val_loss: 0.0295 - val_acc: 0.9959\n",
            "Epoch 494/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0519 - acc: 0.9897 - val_loss: 0.0331 - val_acc: 0.9961\n",
            "Epoch 495/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0469 - acc: 0.9903 - val_loss: 0.0467 - val_acc: 0.9908\n",
            "Epoch 496/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0411 - acc: 0.9914 - val_loss: 0.0338 - val_acc: 0.9944\n",
            "Epoch 497/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0547 - acc: 0.9893 - val_loss: 0.0282 - val_acc: 0.9961\n",
            "Epoch 498/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0549 - acc: 0.9889 - val_loss: 0.0430 - val_acc: 0.9929\n",
            "Epoch 499/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1671 - acc: 0.9786 - val_loss: 0.1218 - val_acc: 0.9804\n",
            "Epoch 500/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0940 - acc: 0.9852 - val_loss: 0.0554 - val_acc: 0.9914\n",
            "Epoch 501/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0539 - acc: 0.9902 - val_loss: 0.0369 - val_acc: 0.9932\n",
            "Epoch 502/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0714 - acc: 0.9887 - val_loss: 0.0297 - val_acc: 0.9968\n",
            "Epoch 503/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0628 - acc: 0.9895 - val_loss: 0.0265 - val_acc: 0.9971\n",
            "Epoch 504/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0502 - acc: 0.9911 - val_loss: 0.0283 - val_acc: 0.9953\n",
            "Epoch 505/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1339 - acc: 0.9846 - val_loss: 0.0748 - val_acc: 0.9901\n",
            "Epoch 506/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0782 - acc: 0.9879 - val_loss: 0.0314 - val_acc: 0.9959\n",
            "Epoch 507/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0832 - acc: 0.9881 - val_loss: 0.0742 - val_acc: 0.9848\n",
            "Epoch 508/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0611 - acc: 0.9902 - val_loss: 0.1383 - val_acc: 0.9800\n",
            "Epoch 509/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0538 - acc: 0.9910 - val_loss: 0.0780 - val_acc: 0.9878\n",
            "Epoch 510/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0595 - acc: 0.9906 - val_loss: 0.0426 - val_acc: 0.9934\n",
            "Epoch 511/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0601 - acc: 0.9896 - val_loss: 0.0287 - val_acc: 0.9961\n",
            "Epoch 512/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0612 - acc: 0.9902 - val_loss: 0.0428 - val_acc: 0.9961\n",
            "Epoch 513/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0904 - acc: 0.9861 - val_loss: 0.0416 - val_acc: 0.9953\n",
            "Epoch 514/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0803 - acc: 0.9877 - val_loss: 0.0286 - val_acc: 0.9960\n",
            "Epoch 515/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0590 - acc: 0.9898 - val_loss: 0.0524 - val_acc: 0.9918\n",
            "Epoch 516/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0559 - acc: 0.9912 - val_loss: 0.0906 - val_acc: 0.9739\n",
            "Epoch 517/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0600 - acc: 0.9907 - val_loss: 0.0682 - val_acc: 0.9843\n",
            "Epoch 518/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0570 - acc: 0.9913 - val_loss: 0.0240 - val_acc: 0.9975\n",
            "Epoch 519/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0845 - acc: 0.9888 - val_loss: 0.2762 - val_acc: 0.9709\n",
            "Epoch 520/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0862 - acc: 0.9872 - val_loss: 0.0230 - val_acc: 0.9973\n",
            "Epoch 521/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0746 - acc: 0.9899 - val_loss: 0.0269 - val_acc: 0.9953\n",
            "Epoch 522/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0815 - acc: 0.9884 - val_loss: 0.4089 - val_acc: 0.9646\n",
            "Epoch 523/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1050 - acc: 0.9845 - val_loss: 0.1029 - val_acc: 0.9829\n",
            "Epoch 524/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.1223 - acc: 0.9850 - val_loss: 0.0402 - val_acc: 0.9942\n",
            "Epoch 525/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0649 - acc: 0.9909 - val_loss: 0.5467 - val_acc: 0.9080\n",
            "Epoch 526/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.1720 - acc: 0.9751 - val_loss: 0.0801 - val_acc: 0.9851\n",
            "Epoch 527/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0613 - acc: 0.9879 - val_loss: 0.0690 - val_acc: 0.9859\n",
            "Epoch 528/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0633 - acc: 0.9890 - val_loss: 0.0523 - val_acc: 0.9914\n",
            "Epoch 529/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0589 - acc: 0.9897 - val_loss: 0.0671 - val_acc: 0.9885\n",
            "Epoch 530/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0481 - acc: 0.9908 - val_loss: 0.0566 - val_acc: 0.9910\n",
            "Epoch 531/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0501 - acc: 0.9911 - val_loss: 0.0370 - val_acc: 0.9953\n",
            "Epoch 532/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0694 - acc: 0.9887 - val_loss: 0.1712 - val_acc: 0.9771\n",
            "Epoch 533/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0501 - acc: 0.9917 - val_loss: 0.0719 - val_acc: 0.9908\n",
            "Epoch 534/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0481 - acc: 0.9922 - val_loss: 0.0274 - val_acc: 0.9956\n",
            "Epoch 535/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0560 - acc: 0.9904 - val_loss: 0.0625 - val_acc: 0.9908\n",
            "Epoch 536/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0479 - acc: 0.9914 - val_loss: 0.0268 - val_acc: 0.9964\n",
            "Epoch 537/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0387 - acc: 0.9926 - val_loss: 0.0337 - val_acc: 0.9952\n",
            "Epoch 538/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0587 - acc: 0.9913 - val_loss: 0.0665 - val_acc: 0.9899\n",
            "Epoch 539/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0721 - acc: 0.9886 - val_loss: 0.0613 - val_acc: 0.9929\n",
            "Epoch 540/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0828 - acc: 0.9873 - val_loss: 0.0414 - val_acc: 0.9939\n",
            "Epoch 541/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0653 - acc: 0.9893 - val_loss: 0.0390 - val_acc: 0.9929\n",
            "Epoch 542/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0645 - acc: 0.9892 - val_loss: 0.0292 - val_acc: 0.9955\n",
            "Epoch 543/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0466 - acc: 0.9917 - val_loss: 0.0268 - val_acc: 0.9966\n",
            "Epoch 544/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0613 - acc: 0.9896 - val_loss: 0.0393 - val_acc: 0.9939\n",
            "Epoch 545/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0519 - acc: 0.9904 - val_loss: 0.0220 - val_acc: 0.9967\n",
            "Epoch 546/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0464 - acc: 0.9928 - val_loss: 0.1902 - val_acc: 0.9432\n",
            "Epoch 547/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0569 - acc: 0.9898 - val_loss: 0.0253 - val_acc: 0.9968\n",
            "Epoch 548/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0778 - acc: 0.9900 - val_loss: 0.0261 - val_acc: 0.9964\n",
            "Epoch 549/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0744 - acc: 0.9881 - val_loss: 0.1762 - val_acc: 0.9761\n",
            "Epoch 550/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1023 - acc: 0.9859 - val_loss: 0.0354 - val_acc: 0.9971\n",
            "Epoch 551/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0571 - acc: 0.9904 - val_loss: 0.0802 - val_acc: 0.9846\n",
            "Epoch 552/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0485 - acc: 0.9910 - val_loss: 0.0304 - val_acc: 0.9964\n",
            "Epoch 553/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0889 - acc: 0.9883 - val_loss: 0.0220 - val_acc: 0.9971\n",
            "Epoch 554/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0701 - acc: 0.9892 - val_loss: 0.1062 - val_acc: 0.9841\n",
            "Epoch 555/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0578 - acc: 0.9905 - val_loss: 0.1924 - val_acc: 0.9765\n",
            "Epoch 556/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0607 - acc: 0.9898 - val_loss: 0.1096 - val_acc: 0.9766\n",
            "Epoch 557/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0970 - acc: 0.9868 - val_loss: 0.0536 - val_acc: 0.9912\n",
            "Epoch 558/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0580 - acc: 0.9899 - val_loss: 0.0530 - val_acc: 0.9915\n",
            "Epoch 559/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0655 - acc: 0.9899 - val_loss: 0.0489 - val_acc: 0.9947\n",
            "Epoch 560/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0816 - acc: 0.9876 - val_loss: 0.0287 - val_acc: 0.9971\n",
            "Epoch 561/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0627 - acc: 0.9903 - val_loss: 0.2580 - val_acc: 0.9531\n",
            "Epoch 562/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0856 - acc: 0.9884 - val_loss: 0.0470 - val_acc: 0.9926\n",
            "Epoch 563/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0431 - acc: 0.9919 - val_loss: 0.0219 - val_acc: 0.9970\n",
            "Epoch 564/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0431 - acc: 0.9924 - val_loss: 0.0231 - val_acc: 0.9963\n",
            "Epoch 565/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0703 - acc: 0.9883 - val_loss: 0.0272 - val_acc: 0.9987\n",
            "Epoch 566/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0686 - acc: 0.9889 - val_loss: 0.2281 - val_acc: 0.9732\n",
            "Epoch 567/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0597 - acc: 0.9896 - val_loss: 0.0248 - val_acc: 0.9972\n",
            "Epoch 568/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0747 - acc: 0.9886 - val_loss: 0.0359 - val_acc: 0.9958\n",
            "Epoch 569/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0666 - acc: 0.9897 - val_loss: 0.0226 - val_acc: 0.9978\n",
            "Epoch 570/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0418 - acc: 0.9930 - val_loss: 0.1069 - val_acc: 0.9750\n",
            "Epoch 571/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0426 - acc: 0.9928 - val_loss: 0.1053 - val_acc: 0.9833\n",
            "Epoch 572/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0563 - acc: 0.9910 - val_loss: 0.0202 - val_acc: 0.9975\n",
            "Epoch 573/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.0483 - acc: 0.9915 - val_loss: 0.0391 - val_acc: 0.9958\n",
            "Epoch 574/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0442 - acc: 0.9929 - val_loss: 0.0571 - val_acc: 0.9914\n",
            "Epoch 575/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0403 - acc: 0.9936 - val_loss: 0.0481 - val_acc: 0.9920\n",
            "Epoch 576/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0742 - acc: 0.9893 - val_loss: 0.0300 - val_acc: 0.9954\n",
            "Epoch 577/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0548 - acc: 0.9910 - val_loss: 0.0650 - val_acc: 0.9901\n",
            "Epoch 578/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0803 - acc: 0.9888 - val_loss: 0.1465 - val_acc: 0.9809\n",
            "Epoch 579/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0886 - acc: 0.9886 - val_loss: 0.0520 - val_acc: 0.9954\n",
            "Epoch 580/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0668 - acc: 0.9887 - val_loss: 0.0415 - val_acc: 0.9940\n",
            "Epoch 581/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0783 - acc: 0.9889 - val_loss: 0.0525 - val_acc: 0.9932\n",
            "Epoch 582/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0631 - acc: 0.9913 - val_loss: 0.0426 - val_acc: 0.9949\n",
            "Epoch 583/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0808 - acc: 0.9895 - val_loss: 0.0756 - val_acc: 0.9852\n",
            "Epoch 584/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0721 - acc: 0.9901 - val_loss: 0.1387 - val_acc: 0.9813\n",
            "Epoch 585/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0587 - acc: 0.9904 - val_loss: 0.0302 - val_acc: 0.9967\n",
            "Epoch 586/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0621 - acc: 0.9907 - val_loss: 0.0340 - val_acc: 0.9964\n",
            "Epoch 587/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0538 - acc: 0.9919 - val_loss: 0.0204 - val_acc: 0.9967\n",
            "Epoch 588/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0460 - acc: 0.9915 - val_loss: 0.0593 - val_acc: 0.9904\n",
            "Epoch 589/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0998 - acc: 0.9852 - val_loss: 0.0968 - val_acc: 0.9867\n",
            "Epoch 590/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0424 - acc: 0.9931 - val_loss: 0.0224 - val_acc: 0.9966\n",
            "Epoch 591/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0739 - acc: 0.9885 - val_loss: 0.0363 - val_acc: 0.9945\n",
            "Epoch 592/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0840 - acc: 0.9882 - val_loss: 0.0845 - val_acc: 0.9880\n",
            "Epoch 593/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0458 - acc: 0.9928 - val_loss: 0.0399 - val_acc: 0.9961\n",
            "Epoch 594/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0473 - acc: 0.9926 - val_loss: 0.0436 - val_acc: 0.9959\n",
            "Epoch 595/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0651 - acc: 0.9905 - val_loss: 0.1229 - val_acc: 0.9812\n",
            "Epoch 596/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0407 - acc: 0.9940 - val_loss: 0.0256 - val_acc: 0.9969\n",
            "Epoch 597/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1080 - acc: 0.9823 - val_loss: 0.0372 - val_acc: 0.9931\n",
            "Epoch 598/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0574 - acc: 0.9908 - val_loss: 0.3116 - val_acc: 0.9693\n",
            "Epoch 599/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0623 - acc: 0.9906 - val_loss: 0.0400 - val_acc: 0.9933\n",
            "Epoch 600/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0523 - acc: 0.9908 - val_loss: 0.0208 - val_acc: 0.9980\n",
            "Epoch 601/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0439 - acc: 0.9928 - val_loss: 0.1743 - val_acc: 0.9784\n",
            "Epoch 602/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0846 - acc: 0.9882 - val_loss: 0.0330 - val_acc: 0.9943\n",
            "Epoch 603/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0546 - acc: 0.9919 - val_loss: 0.0268 - val_acc: 0.9969\n",
            "Epoch 604/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0852 - acc: 0.9870 - val_loss: 0.0276 - val_acc: 0.9967\n",
            "Epoch 605/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0733 - acc: 0.9886 - val_loss: 0.0305 - val_acc: 0.9952\n",
            "Epoch 606/1000\n",
            "54924/54924 [==============================] - 3s 56us/step - loss: 0.0366 - acc: 0.9942 - val_loss: 0.0553 - val_acc: 0.9897\n",
            "Epoch 607/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0527 - acc: 0.9920 - val_loss: 0.1045 - val_acc: 0.9836\n",
            "Epoch 608/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0433 - acc: 0.9922 - val_loss: 0.0461 - val_acc: 0.9925\n",
            "Epoch 609/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0679 - acc: 0.9893 - val_loss: 0.1198 - val_acc: 0.9819\n",
            "Epoch 610/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0610 - acc: 0.9908 - val_loss: 0.0648 - val_acc: 0.9926\n",
            "Epoch 611/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0432 - acc: 0.9916 - val_loss: 0.0305 - val_acc: 0.9952\n",
            "Epoch 612/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0565 - acc: 0.9908 - val_loss: 0.0325 - val_acc: 0.9948\n",
            "Epoch 613/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0780 - acc: 0.9883 - val_loss: 0.0283 - val_acc: 0.9965\n",
            "Epoch 614/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0547 - acc: 0.9914 - val_loss: 0.0744 - val_acc: 0.9890\n",
            "Epoch 615/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0414 - acc: 0.9932 - val_loss: 0.0426 - val_acc: 0.9934\n",
            "Epoch 616/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0288 - acc: 0.9950 - val_loss: 0.0147 - val_acc: 0.9977\n",
            "Epoch 617/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0600 - acc: 0.9915 - val_loss: 0.0381 - val_acc: 0.9953\n",
            "Epoch 618/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0301 - acc: 0.9944 - val_loss: 0.0627 - val_acc: 0.9905\n",
            "Epoch 619/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0602 - acc: 0.9909 - val_loss: 0.4803 - val_acc: 0.8932\n",
            "Epoch 620/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0634 - acc: 0.9898 - val_loss: 0.0176 - val_acc: 0.9982\n",
            "Epoch 621/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0302 - acc: 0.9947 - val_loss: 0.1103 - val_acc: 0.9841\n",
            "Epoch 622/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1056 - acc: 0.9895 - val_loss: 0.5918 - val_acc: 0.9598\n",
            "Epoch 623/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.3798 - acc: 0.9701 - val_loss: 0.0508 - val_acc: 0.9929\n",
            "Epoch 624/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0834 - acc: 0.9879 - val_loss: 0.1719 - val_acc: 0.9819\n",
            "Epoch 625/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0529 - acc: 0.9916 - val_loss: 0.4958 - val_acc: 0.9120\n",
            "Epoch 626/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.2739 - acc: 0.9723 - val_loss: 0.0544 - val_acc: 0.9932\n",
            "Epoch 627/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1099 - acc: 0.9863 - val_loss: 0.0782 - val_acc: 0.9901\n",
            "Epoch 628/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.1454 - acc: 0.9859 - val_loss: 0.4262 - val_acc: 0.9664\n",
            "Epoch 629/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0919 - acc: 0.9892 - val_loss: 0.0356 - val_acc: 0.9947\n",
            "Epoch 630/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0691 - acc: 0.9902 - val_loss: 0.2940 - val_acc: 0.9719\n",
            "Epoch 631/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0879 - acc: 0.9893 - val_loss: 0.0378 - val_acc: 0.9942\n",
            "Epoch 632/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0646 - acc: 0.9909 - val_loss: 0.0289 - val_acc: 0.9958\n",
            "Epoch 633/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0478 - acc: 0.9925 - val_loss: 0.0233 - val_acc: 0.9965\n",
            "Epoch 634/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.2387 - acc: 0.9783 - val_loss: 0.0347 - val_acc: 0.9947\n",
            "Epoch 635/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0754 - acc: 0.9895 - val_loss: 0.1055 - val_acc: 0.9872\n",
            "Epoch 636/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0890 - acc: 0.9868 - val_loss: 0.0306 - val_acc: 0.9949\n",
            "Epoch 637/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0491 - acc: 0.9918 - val_loss: 0.0195 - val_acc: 0.9964\n",
            "Epoch 638/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0334 - acc: 0.9937 - val_loss: 0.0307 - val_acc: 0.9948\n",
            "Epoch 639/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0543 - acc: 0.9902 - val_loss: 0.2250 - val_acc: 0.9760\n",
            "Epoch 640/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0646 - acc: 0.9890 - val_loss: 0.2272 - val_acc: 0.9455\n",
            "Epoch 641/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0651 - acc: 0.9880 - val_loss: 0.0414 - val_acc: 0.9927\n",
            "Epoch 642/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0607 - acc: 0.9893 - val_loss: 0.0308 - val_acc: 0.9967\n",
            "Epoch 643/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0419 - acc: 0.9916 - val_loss: 0.0272 - val_acc: 0.9956\n",
            "Epoch 644/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0406 - acc: 0.9928 - val_loss: 0.0229 - val_acc: 0.9972\n",
            "Epoch 645/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0482 - acc: 0.9913 - val_loss: 0.2201 - val_acc: 0.9424\n",
            "Epoch 646/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0585 - acc: 0.9905 - val_loss: 0.0450 - val_acc: 0.9922\n",
            "Epoch 647/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0545 - acc: 0.9909 - val_loss: 0.1487 - val_acc: 0.9596\n",
            "Epoch 648/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0491 - acc: 0.9909 - val_loss: 0.0817 - val_acc: 0.9886\n",
            "Epoch 649/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0590 - acc: 0.9897 - val_loss: 0.0326 - val_acc: 0.9944\n",
            "Epoch 650/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0476 - acc: 0.9916 - val_loss: 0.0497 - val_acc: 0.9916\n",
            "Epoch 651/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0474 - acc: 0.9915 - val_loss: 0.0683 - val_acc: 0.9892\n",
            "Epoch 652/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0540 - acc: 0.9916 - val_loss: 0.0766 - val_acc: 0.9888\n",
            "Epoch 653/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0428 - acc: 0.9924 - val_loss: 0.0314 - val_acc: 0.9958\n",
            "Epoch 654/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0544 - acc: 0.9903 - val_loss: 0.0258 - val_acc: 0.9966\n",
            "Epoch 655/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0553 - acc: 0.9908 - val_loss: 0.0341 - val_acc: 0.9945\n",
            "Epoch 656/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0542 - acc: 0.9902 - val_loss: 0.0267 - val_acc: 0.9967\n",
            "Epoch 657/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0449 - acc: 0.9920 - val_loss: 0.0218 - val_acc: 0.9969\n",
            "Epoch 658/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0640 - acc: 0.9909 - val_loss: 0.0351 - val_acc: 0.9950\n",
            "Epoch 659/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0466 - acc: 0.9920 - val_loss: 0.0587 - val_acc: 0.9906\n",
            "Epoch 660/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0567 - acc: 0.9910 - val_loss: 0.5086 - val_acc: 0.9158\n",
            "Epoch 661/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0650 - acc: 0.9896 - val_loss: 0.0670 - val_acc: 0.9898\n",
            "Epoch 662/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0509 - acc: 0.9915 - val_loss: 0.0241 - val_acc: 0.9972\n",
            "Epoch 663/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0849 - acc: 0.9869 - val_loss: 0.0691 - val_acc: 0.9886\n",
            "Epoch 664/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0497 - acc: 0.9909 - val_loss: 0.0231 - val_acc: 0.9969\n",
            "Epoch 665/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0534 - acc: 0.9911 - val_loss: 0.0592 - val_acc: 0.9908\n",
            "Epoch 666/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0492 - acc: 0.9921 - val_loss: 0.0302 - val_acc: 0.9954\n",
            "Epoch 667/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0455 - acc: 0.9922 - val_loss: 0.0519 - val_acc: 0.9908\n",
            "Epoch 668/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0448 - acc: 0.9913 - val_loss: 0.0225 - val_acc: 0.9967\n",
            "Epoch 669/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.0447 - acc: 0.9916 - val_loss: 0.0198 - val_acc: 0.9968\n",
            "Epoch 670/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0808 - acc: 0.9888 - val_loss: 0.0943 - val_acc: 0.9775\n",
            "Epoch 671/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0372 - acc: 0.9937 - val_loss: 0.0282 - val_acc: 0.9959\n",
            "Epoch 672/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0427 - acc: 0.9927 - val_loss: 0.0507 - val_acc: 0.9926\n",
            "Epoch 673/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0445 - acc: 0.9923 - val_loss: 0.0621 - val_acc: 0.9873\n",
            "Epoch 674/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0553 - acc: 0.9914 - val_loss: 0.0454 - val_acc: 0.9937\n",
            "Epoch 675/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0537 - acc: 0.9920 - val_loss: 0.0235 - val_acc: 0.9967\n",
            "Epoch 676/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0363 - acc: 0.9936 - val_loss: 0.0637 - val_acc: 0.9905\n",
            "Epoch 677/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0384 - acc: 0.9935 - val_loss: 0.0898 - val_acc: 0.9868\n",
            "Epoch 678/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0367 - acc: 0.9938 - val_loss: 0.3286 - val_acc: 0.9708\n",
            "Epoch 679/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0518 - acc: 0.9926 - val_loss: 0.0256 - val_acc: 0.9970\n",
            "Epoch 680/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0438 - acc: 0.9922 - val_loss: 0.0478 - val_acc: 0.9926\n",
            "Epoch 681/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0532 - acc: 0.9918 - val_loss: 0.0405 - val_acc: 0.9938\n",
            "Epoch 682/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0417 - acc: 0.9930 - val_loss: 0.0321 - val_acc: 0.9953\n",
            "Epoch 683/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0557 - acc: 0.9910 - val_loss: 0.1402 - val_acc: 0.9816\n",
            "Epoch 684/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0473 - acc: 0.9934 - val_loss: 0.0268 - val_acc: 0.9960\n",
            "Epoch 685/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0616 - acc: 0.9903 - val_loss: 0.0508 - val_acc: 0.9897\n",
            "Epoch 686/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0378 - acc: 0.9932 - val_loss: 0.0545 - val_acc: 0.9915\n",
            "Epoch 687/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0601 - acc: 0.9909 - val_loss: 0.0569 - val_acc: 0.9911\n",
            "Epoch 688/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0415 - acc: 0.9922 - val_loss: 0.0334 - val_acc: 0.9950\n",
            "Epoch 689/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0436 - acc: 0.9942 - val_loss: 0.0178 - val_acc: 0.9982\n",
            "Epoch 690/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0680 - acc: 0.9897 - val_loss: 0.0758 - val_acc: 0.9889\n",
            "Epoch 691/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0681 - acc: 0.9898 - val_loss: 0.0646 - val_acc: 0.9869\n",
            "Epoch 692/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0494 - acc: 0.9920 - val_loss: 0.0205 - val_acc: 0.9967\n",
            "Epoch 693/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.0581 - acc: 0.9911 - val_loss: 0.0881 - val_acc: 0.9878\n",
            "Epoch 694/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0911 - acc: 0.9891 - val_loss: 0.0364 - val_acc: 0.9948\n",
            "Epoch 695/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0408 - acc: 0.9937 - val_loss: 0.0230 - val_acc: 0.9969\n",
            "Epoch 696/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0376 - acc: 0.9937 - val_loss: 0.0171 - val_acc: 0.9974\n",
            "Epoch 697/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0666 - acc: 0.9893 - val_loss: 0.0409 - val_acc: 0.9940\n",
            "Epoch 698/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0492 - acc: 0.9916 - val_loss: 0.0542 - val_acc: 0.9900\n",
            "Epoch 699/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0677 - acc: 0.9902 - val_loss: 0.0611 - val_acc: 0.9888\n",
            "Epoch 700/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1017 - acc: 0.9890 - val_loss: 0.0350 - val_acc: 0.9959\n",
            "Epoch 701/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0353 - acc: 0.9944 - val_loss: 0.0175 - val_acc: 0.9983\n",
            "Epoch 702/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0608 - acc: 0.9894 - val_loss: 0.0265 - val_acc: 0.9963\n",
            "Epoch 703/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0702 - acc: 0.9888 - val_loss: 0.0426 - val_acc: 0.9930\n",
            "Epoch 704/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0279 - acc: 0.9954 - val_loss: 0.0225 - val_acc: 0.9975\n",
            "Epoch 705/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0367 - acc: 0.9942 - val_loss: 0.0301 - val_acc: 0.9952\n",
            "Epoch 706/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0715 - acc: 0.9889 - val_loss: 0.1432 - val_acc: 0.9824\n",
            "Epoch 707/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0476 - acc: 0.9916 - val_loss: 0.0405 - val_acc: 0.9934\n",
            "Epoch 708/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0428 - acc: 0.9930 - val_loss: 0.0300 - val_acc: 0.9966\n",
            "Epoch 709/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0453 - acc: 0.9926 - val_loss: 0.0186 - val_acc: 0.9973\n",
            "Epoch 710/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0344 - acc: 0.9935 - val_loss: 0.0207 - val_acc: 0.9967\n",
            "Epoch 711/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0636 - acc: 0.9904 - val_loss: 0.2767 - val_acc: 0.9278\n",
            "Epoch 712/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0439 - acc: 0.9919 - val_loss: 0.0226 - val_acc: 0.9964\n",
            "Epoch 713/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0283 - acc: 0.9950 - val_loss: 0.0191 - val_acc: 0.9972\n",
            "Epoch 714/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0487 - acc: 0.9912 - val_loss: 0.0500 - val_acc: 0.9909\n",
            "Epoch 715/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0380 - acc: 0.9945 - val_loss: 0.0214 - val_acc: 0.9975\n",
            "Epoch 716/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0437 - acc: 0.9914 - val_loss: 0.0219 - val_acc: 0.9963\n",
            "Epoch 717/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0521 - acc: 0.9904 - val_loss: 0.0643 - val_acc: 0.9891\n",
            "Epoch 718/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0464 - acc: 0.9924 - val_loss: 0.0157 - val_acc: 0.9980\n",
            "Epoch 719/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0640 - acc: 0.9916 - val_loss: 0.0624 - val_acc: 0.9908\n",
            "Epoch 720/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0394 - acc: 0.9935 - val_loss: 0.0323 - val_acc: 0.9948\n",
            "Epoch 721/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0592 - acc: 0.9911 - val_loss: 0.0334 - val_acc: 0.9958\n",
            "Epoch 722/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0397 - acc: 0.9942 - val_loss: 0.0651 - val_acc: 0.9870\n",
            "Epoch 723/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0643 - acc: 0.9898 - val_loss: 0.0273 - val_acc: 0.9954\n",
            "Epoch 724/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0749 - acc: 0.9904 - val_loss: 0.0835 - val_acc: 0.9885\n",
            "Epoch 725/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0463 - acc: 0.9920 - val_loss: 0.0154 - val_acc: 0.9977\n",
            "Epoch 726/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0675 - acc: 0.9896 - val_loss: 0.0170 - val_acc: 0.9980\n",
            "Epoch 727/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0425 - acc: 0.9926 - val_loss: 0.0198 - val_acc: 0.9969\n",
            "Epoch 728/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0548 - acc: 0.9918 - val_loss: 0.0893 - val_acc: 0.9873\n",
            "Epoch 729/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.2973 - acc: 0.9756 - val_loss: 0.2716 - val_acc: 0.9722\n",
            "Epoch 730/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0639 - acc: 0.9899 - val_loss: 0.0197 - val_acc: 0.9980\n",
            "Epoch 731/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0662 - acc: 0.9926 - val_loss: 0.0216 - val_acc: 0.9970\n",
            "Epoch 732/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0461 - acc: 0.9931 - val_loss: 0.0191 - val_acc: 0.9983\n",
            "Epoch 733/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0276 - acc: 0.9952 - val_loss: 0.0154 - val_acc: 0.9983\n",
            "Epoch 734/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0260 - acc: 0.9955 - val_loss: 0.0303 - val_acc: 0.9959\n",
            "Epoch 735/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0806 - acc: 0.9895 - val_loss: 0.0588 - val_acc: 0.9888\n",
            "Epoch 736/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0567 - acc: 0.9916 - val_loss: 0.0390 - val_acc: 0.9948\n",
            "Epoch 737/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0839 - acc: 0.9881 - val_loss: 0.0230 - val_acc: 0.9975\n",
            "Epoch 738/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0569 - acc: 0.9912 - val_loss: 0.0256 - val_acc: 0.9967\n",
            "Epoch 739/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0699 - acc: 0.9901 - val_loss: 0.0787 - val_acc: 0.9885\n",
            "Epoch 740/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0788 - acc: 0.9893 - val_loss: 0.1334 - val_acc: 0.9827\n",
            "Epoch 741/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0745 - acc: 0.9912 - val_loss: 0.1405 - val_acc: 0.9832\n",
            "Epoch 742/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0386 - acc: 0.9939 - val_loss: 0.0160 - val_acc: 0.9985\n",
            "Epoch 743/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0373 - acc: 0.9945 - val_loss: 0.0168 - val_acc: 0.9984\n",
            "Epoch 744/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0405 - acc: 0.9940 - val_loss: 0.1185 - val_acc: 0.9852\n",
            "Epoch 745/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0378 - acc: 0.9938 - val_loss: 0.0654 - val_acc: 0.9906\n",
            "Epoch 746/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0438 - acc: 0.9926 - val_loss: 0.0161 - val_acc: 0.9973\n",
            "Epoch 747/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.1128 - acc: 0.9880 - val_loss: 0.0286 - val_acc: 0.9962\n",
            "Epoch 748/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0404 - acc: 0.9934 - val_loss: 0.1108 - val_acc: 0.9856\n",
            "Epoch 749/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0489 - acc: 0.9927 - val_loss: 0.0493 - val_acc: 0.9937\n",
            "Epoch 750/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0466 - acc: 0.9934 - val_loss: 0.0145 - val_acc: 0.9980\n",
            "Epoch 751/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0900 - acc: 0.9905 - val_loss: 0.0747 - val_acc: 0.9900\n",
            "Epoch 752/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0359 - acc: 0.9950 - val_loss: 0.0751 - val_acc: 0.9900\n",
            "Epoch 753/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0315 - acc: 0.9948 - val_loss: 0.0454 - val_acc: 0.9937\n",
            "Epoch 754/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0982 - acc: 0.9878 - val_loss: 0.2000 - val_acc: 0.9783\n",
            "Epoch 755/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0331 - acc: 0.9954 - val_loss: 0.0158 - val_acc: 0.9977\n",
            "Epoch 756/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0512 - acc: 0.9922 - val_loss: 0.0551 - val_acc: 0.9923\n",
            "Epoch 757/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0611 - acc: 0.9930 - val_loss: 0.0157 - val_acc: 0.9984\n",
            "Epoch 758/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0968 - acc: 0.9884 - val_loss: 0.0499 - val_acc: 0.9945\n",
            "Epoch 759/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0404 - acc: 0.9945 - val_loss: 0.0279 - val_acc: 0.9969\n",
            "Epoch 760/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0507 - acc: 0.9921 - val_loss: 0.0385 - val_acc: 0.9949\n",
            "Epoch 761/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0340 - acc: 0.9945 - val_loss: 0.0169 - val_acc: 0.9981\n",
            "Epoch 762/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0650 - acc: 0.9925 - val_loss: 0.0374 - val_acc: 0.9953\n",
            "Epoch 763/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0405 - acc: 0.9931 - val_loss: 0.0293 - val_acc: 0.9965\n",
            "Epoch 764/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0655 - acc: 0.9917 - val_loss: 0.0366 - val_acc: 0.9951\n",
            "Epoch 765/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0495 - acc: 0.9919 - val_loss: 0.0341 - val_acc: 0.9955\n",
            "Epoch 766/1000\n",
            "54924/54924 [==============================] - 2s 40us/step - loss: 0.0633 - acc: 0.9927 - val_loss: 0.0511 - val_acc: 0.9910\n",
            "Epoch 767/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0301 - acc: 0.9955 - val_loss: 0.0389 - val_acc: 0.9931\n",
            "Epoch 768/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0308 - acc: 0.9945 - val_loss: 0.0221 - val_acc: 0.9969\n",
            "Epoch 769/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0500 - acc: 0.9930 - val_loss: 0.0235 - val_acc: 0.9977\n",
            "Epoch 770/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1115 - acc: 0.9841 - val_loss: 0.0225 - val_acc: 0.9975\n",
            "Epoch 771/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0795 - acc: 0.9883 - val_loss: 0.0307 - val_acc: 0.9963\n",
            "Epoch 772/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0611 - acc: 0.9902 - val_loss: 0.0372 - val_acc: 0.9937\n",
            "Epoch 773/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0345 - acc: 0.9946 - val_loss: 0.0126 - val_acc: 0.9985\n",
            "Epoch 774/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0386 - acc: 0.9937 - val_loss: 0.0262 - val_acc: 0.9962\n",
            "Epoch 775/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0584 - acc: 0.9918 - val_loss: 0.0153 - val_acc: 0.9986\n",
            "Epoch 776/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0283 - acc: 0.9951 - val_loss: 0.0126 - val_acc: 0.9984\n",
            "Epoch 777/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0481 - acc: 0.9924 - val_loss: 0.0618 - val_acc: 0.9913\n",
            "Epoch 778/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0946 - acc: 0.9855 - val_loss: 0.0131 - val_acc: 0.9991\n",
            "Epoch 779/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0344 - acc: 0.9946 - val_loss: 0.0146 - val_acc: 0.9977\n",
            "Epoch 780/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0351 - acc: 0.9934 - val_loss: 0.0746 - val_acc: 0.9867\n",
            "Epoch 781/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0637 - acc: 0.9908 - val_loss: 0.0331 - val_acc: 0.9955\n",
            "Epoch 782/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0431 - acc: 0.9932 - val_loss: 0.0855 - val_acc: 0.9887\n",
            "Epoch 783/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0452 - acc: 0.9926 - val_loss: 0.0222 - val_acc: 0.9967\n",
            "Epoch 784/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0640 - acc: 0.9912 - val_loss: 0.0843 - val_acc: 0.9848\n",
            "Epoch 785/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0284 - acc: 0.9951 - val_loss: 0.1273 - val_acc: 0.9831\n",
            "Epoch 786/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0475 - acc: 0.9935 - val_loss: 0.0172 - val_acc: 0.9974\n",
            "Epoch 787/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0551 - acc: 0.9927 - val_loss: 0.0168 - val_acc: 0.9982\n",
            "Epoch 788/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0707 - acc: 0.9907 - val_loss: 0.0403 - val_acc: 0.9945\n",
            "Epoch 789/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0408 - acc: 0.9945 - val_loss: 0.0144 - val_acc: 0.9983\n",
            "Epoch 790/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0370 - acc: 0.9936 - val_loss: 0.0179 - val_acc: 0.9972\n",
            "Epoch 791/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0463 - acc: 0.9930 - val_loss: 0.0397 - val_acc: 0.9941\n",
            "Epoch 792/1000\n",
            "54924/54924 [==============================] - 2s 46us/step - loss: 0.0455 - acc: 0.9931 - val_loss: 0.0175 - val_acc: 0.9976\n",
            "Epoch 793/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0319 - acc: 0.9947 - val_loss: 0.0615 - val_acc: 0.9911\n",
            "Epoch 794/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0390 - acc: 0.9939 - val_loss: 0.1092 - val_acc: 0.9862\n",
            "Epoch 795/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0906 - acc: 0.9873 - val_loss: 0.0366 - val_acc: 0.9950\n",
            "Epoch 796/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0357 - acc: 0.9946 - val_loss: 0.0174 - val_acc: 0.9980\n",
            "Epoch 797/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0726 - acc: 0.9906 - val_loss: 0.1419 - val_acc: 0.9824\n",
            "Epoch 798/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.1790 - acc: 0.9822 - val_loss: 0.1452 - val_acc: 0.9799\n",
            "Epoch 799/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0429 - acc: 0.9938 - val_loss: 0.1999 - val_acc: 0.9782\n",
            "Epoch 800/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0458 - acc: 0.9929 - val_loss: 0.0220 - val_acc: 0.9973\n",
            "Epoch 801/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0630 - acc: 0.9913 - val_loss: 0.1833 - val_acc: 0.9788\n",
            "Epoch 802/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0788 - acc: 0.9888 - val_loss: 0.0638 - val_acc: 0.9908\n",
            "Epoch 803/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0247 - acc: 0.9960 - val_loss: 0.1234 - val_acc: 0.9747\n",
            "Epoch 804/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0380 - acc: 0.9939 - val_loss: 0.1197 - val_acc: 0.9842\n",
            "Epoch 805/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.1124 - acc: 0.9881 - val_loss: 0.2582 - val_acc: 0.9745\n",
            "Epoch 806/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0517 - acc: 0.9920 - val_loss: 0.0311 - val_acc: 0.9961\n",
            "Epoch 807/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0347 - acc: 0.9946 - val_loss: 0.2000 - val_acc: 0.9786\n",
            "Epoch 808/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0332 - acc: 0.9956 - val_loss: 0.0184 - val_acc: 0.9972\n",
            "Epoch 809/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0402 - acc: 0.9941 - val_loss: 0.0420 - val_acc: 0.9940\n",
            "Epoch 810/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0427 - acc: 0.9937 - val_loss: 0.0117 - val_acc: 0.9988\n",
            "Epoch 811/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0536 - acc: 0.9926 - val_loss: 0.3154 - val_acc: 0.9725\n",
            "Epoch 812/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0456 - acc: 0.9933 - val_loss: 0.0379 - val_acc: 0.9946\n",
            "Epoch 813/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0656 - acc: 0.9915 - val_loss: 0.0231 - val_acc: 0.9967\n",
            "Epoch 814/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0268 - acc: 0.9956 - val_loss: 0.0091 - val_acc: 0.9988\n",
            "Epoch 815/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0699 - acc: 0.9906 - val_loss: 0.0406 - val_acc: 0.9944\n",
            "Epoch 816/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0594 - acc: 0.9915 - val_loss: 0.0919 - val_acc: 0.9873\n",
            "Epoch 817/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0360 - acc: 0.9944 - val_loss: 0.0532 - val_acc: 0.9921\n",
            "Epoch 818/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0490 - acc: 0.9928 - val_loss: 0.0333 - val_acc: 0.9953\n",
            "Epoch 819/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0426 - acc: 0.9936 - val_loss: 0.0160 - val_acc: 0.9983\n",
            "Epoch 820/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0400 - acc: 0.9947 - val_loss: 0.0211 - val_acc: 0.9970\n",
            "Epoch 821/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0438 - acc: 0.9933 - val_loss: 0.1141 - val_acc: 0.9673\n",
            "Epoch 822/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0407 - acc: 0.9935 - val_loss: 0.0422 - val_acc: 0.9940\n",
            "Epoch 823/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0567 - acc: 0.9909 - val_loss: 0.0413 - val_acc: 0.9929\n",
            "Epoch 824/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0699 - acc: 0.9928 - val_loss: 0.2383 - val_acc: 0.9768\n",
            "Epoch 825/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0377 - acc: 0.9948 - val_loss: 0.0120 - val_acc: 0.9991\n",
            "Epoch 826/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0536 - acc: 0.9918 - val_loss: 0.0720 - val_acc: 0.9897\n",
            "Epoch 827/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0549 - acc: 0.9930 - val_loss: 0.0167 - val_acc: 0.9978\n",
            "Epoch 828/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0484 - acc: 0.9924 - val_loss: 0.0655 - val_acc: 0.9910\n",
            "Epoch 829/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0319 - acc: 0.9948 - val_loss: 0.0111 - val_acc: 0.9988\n",
            "Epoch 830/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0654 - acc: 0.9912 - val_loss: 0.0321 - val_acc: 0.9958\n",
            "Epoch 831/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0282 - acc: 0.9956 - val_loss: 0.0250 - val_acc: 0.9963\n",
            "Epoch 832/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0425 - acc: 0.9931 - val_loss: 0.0931 - val_acc: 0.9878\n",
            "Epoch 833/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0645 - acc: 0.9926 - val_loss: 0.0533 - val_acc: 0.9930\n",
            "Epoch 834/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0415 - acc: 0.9938 - val_loss: 0.0454 - val_acc: 0.9936\n",
            "Epoch 835/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0304 - acc: 0.9951 - val_loss: 0.0505 - val_acc: 0.9926\n",
            "Epoch 836/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.1056 - acc: 0.9882 - val_loss: 0.0144 - val_acc: 0.9988\n",
            "Epoch 837/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0444 - acc: 0.9926 - val_loss: 0.0546 - val_acc: 0.9919\n",
            "Epoch 838/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0692 - acc: 0.9910 - val_loss: 0.0113 - val_acc: 0.9991\n",
            "Epoch 839/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.0458 - acc: 0.9932 - val_loss: 0.0249 - val_acc: 0.9980\n",
            "Epoch 840/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0433 - acc: 0.9939 - val_loss: 0.0680 - val_acc: 0.9905\n",
            "Epoch 841/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0309 - acc: 0.9950 - val_loss: 0.0126 - val_acc: 0.9985\n",
            "Epoch 842/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0735 - acc: 0.9903 - val_loss: 0.1174 - val_acc: 0.9849\n",
            "Epoch 843/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0588 - acc: 0.9935 - val_loss: 0.0411 - val_acc: 0.9939\n",
            "Epoch 844/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0621 - acc: 0.9930 - val_loss: 0.0174 - val_acc: 0.9988\n",
            "Epoch 845/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0283 - acc: 0.9954 - val_loss: 0.0103 - val_acc: 0.9990\n",
            "Epoch 846/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0515 - acc: 0.9925 - val_loss: 0.0108 - val_acc: 0.9989\n",
            "Epoch 847/1000\n",
            "54924/54924 [==============================] - 3s 57us/step - loss: 0.0440 - acc: 0.9931 - val_loss: 0.0154 - val_acc: 0.9985\n",
            "Epoch 848/1000\n",
            "54924/54924 [==============================] - 3s 58us/step - loss: 0.0447 - acc: 0.9933 - val_loss: 0.0984 - val_acc: 0.9873\n",
            "Epoch 849/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0839 - acc: 0.9882 - val_loss: 0.0312 - val_acc: 0.9963\n",
            "Epoch 850/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0433 - acc: 0.9935 - val_loss: 0.0256 - val_acc: 0.9969\n",
            "Epoch 851/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0349 - acc: 0.9947 - val_loss: 0.1293 - val_acc: 0.9843\n",
            "Epoch 852/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0494 - acc: 0.9927 - val_loss: 0.0292 - val_acc: 0.9957\n",
            "Epoch 853/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0457 - acc: 0.9924 - val_loss: 0.0186 - val_acc: 0.9980\n",
            "Epoch 854/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0586 - acc: 0.9922 - val_loss: 0.0322 - val_acc: 0.9963\n",
            "Epoch 855/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0411 - acc: 0.9938 - val_loss: 0.0429 - val_acc: 0.9924\n",
            "Epoch 856/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0677 - acc: 0.9886 - val_loss: 0.0883 - val_acc: 0.9884\n",
            "Epoch 857/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0456 - acc: 0.9948 - val_loss: 0.0155 - val_acc: 0.9984\n",
            "Epoch 858/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0410 - acc: 0.9954 - val_loss: 0.4224 - val_acc: 0.9677\n",
            "Epoch 859/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0549 - acc: 0.9928 - val_loss: 1.0137 - val_acc: 0.8632\n",
            "Epoch 860/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0766 - acc: 0.9896 - val_loss: 0.0653 - val_acc: 0.9908\n",
            "Epoch 861/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0276 - acc: 0.9958 - val_loss: 0.0237 - val_acc: 0.9969\n",
            "Epoch 862/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0381 - acc: 0.9945 - val_loss: 0.0510 - val_acc: 0.9928\n",
            "Epoch 863/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0475 - acc: 0.9932 - val_loss: 0.1583 - val_acc: 0.9818\n",
            "Epoch 864/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0803 - acc: 0.9898 - val_loss: 0.0338 - val_acc: 0.9962\n",
            "Epoch 865/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0291 - acc: 0.9953 - val_loss: 0.0486 - val_acc: 0.9933\n",
            "Epoch 866/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.0870 - acc: 0.9896 - val_loss: 0.0308 - val_acc: 0.9958\n",
            "Epoch 867/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.0201 - acc: 0.9971 - val_loss: 0.0427 - val_acc: 0.9940\n",
            "Epoch 868/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0544 - acc: 0.9936 - val_loss: 0.3915 - val_acc: 0.9687\n",
            "Epoch 869/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0956 - acc: 0.9880 - val_loss: 0.0791 - val_acc: 0.9886\n",
            "Epoch 870/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0784 - acc: 0.9906 - val_loss: 0.0379 - val_acc: 0.9956\n",
            "Epoch 871/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0392 - acc: 0.9943 - val_loss: 0.0659 - val_acc: 0.9853\n",
            "Epoch 872/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.0900 - acc: 0.9892 - val_loss: 0.0733 - val_acc: 0.9831\n",
            "Epoch 873/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0437 - acc: 0.9937 - val_loss: 0.0234 - val_acc: 0.9967\n",
            "Epoch 874/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0686 - acc: 0.9904 - val_loss: 0.0803 - val_acc: 0.9853\n",
            "Epoch 875/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0596 - acc: 0.9913 - val_loss: 0.0229 - val_acc: 0.9969\n",
            "Epoch 876/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0903 - acc: 0.9877 - val_loss: 0.0261 - val_acc: 0.9974\n",
            "Epoch 877/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0437 - acc: 0.9941 - val_loss: 0.1207 - val_acc: 0.9843\n",
            "Epoch 878/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0407 - acc: 0.9941 - val_loss: 0.0155 - val_acc: 0.9985\n",
            "Epoch 879/1000\n",
            "54924/54924 [==============================] - 2s 41us/step - loss: 0.0541 - acc: 0.9920 - val_loss: 0.0385 - val_acc: 0.9939\n",
            "Epoch 880/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0169 - acc: 0.9973 - val_loss: 0.0212 - val_acc: 0.9959\n",
            "Epoch 881/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0625 - acc: 0.9907 - val_loss: 0.0495 - val_acc: 0.9905\n",
            "Epoch 882/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0376 - acc: 0.9948 - val_loss: 0.0339 - val_acc: 0.9948\n",
            "Epoch 883/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0871 - acc: 0.9891 - val_loss: 0.0504 - val_acc: 0.9905\n",
            "Epoch 884/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0381 - acc: 0.9943 - val_loss: 0.0212 - val_acc: 0.9977\n",
            "Epoch 885/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0498 - acc: 0.9930 - val_loss: 0.0230 - val_acc: 0.9972\n",
            "Epoch 886/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0388 - acc: 0.9936 - val_loss: 0.0251 - val_acc: 0.9975\n",
            "Epoch 887/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0389 - acc: 0.9951 - val_loss: 0.0260 - val_acc: 0.9967\n",
            "Epoch 888/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0390 - acc: 0.9941 - val_loss: 0.0397 - val_acc: 0.9943\n",
            "Epoch 889/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0463 - acc: 0.9926 - val_loss: 0.0244 - val_acc: 0.9975\n",
            "Epoch 890/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0436 - acc: 0.9938 - val_loss: 0.0103 - val_acc: 0.9991\n",
            "Epoch 891/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0268 - acc: 0.9959 - val_loss: 0.0311 - val_acc: 0.9958\n",
            "Epoch 892/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0366 - acc: 0.9943 - val_loss: 0.0479 - val_acc: 0.9934\n",
            "Epoch 893/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0286 - acc: 0.9958 - val_loss: 0.0522 - val_acc: 0.9908\n",
            "Epoch 894/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0502 - acc: 0.9932 - val_loss: 0.0121 - val_acc: 0.9983\n",
            "Epoch 895/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.1564 - acc: 0.9815 - val_loss: 0.0265 - val_acc: 0.9969\n",
            "Epoch 896/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0343 - acc: 0.9952 - val_loss: 0.0274 - val_acc: 0.9966\n",
            "Epoch 897/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0350 - acc: 0.9946 - val_loss: 0.0129 - val_acc: 0.9985\n",
            "Epoch 898/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0726 - acc: 0.9905 - val_loss: 0.0260 - val_acc: 0.9970\n",
            "Epoch 899/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0651 - acc: 0.9901 - val_loss: 0.1588 - val_acc: 0.9808\n",
            "Epoch 900/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0422 - acc: 0.9928 - val_loss: 0.0166 - val_acc: 0.9977\n",
            "Epoch 901/1000\n",
            "54924/54924 [==============================] - 3s 55us/step - loss: 0.0308 - acc: 0.9952 - val_loss: 0.0433 - val_acc: 0.9931\n",
            "Epoch 902/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0411 - acc: 0.9931 - val_loss: 0.0760 - val_acc: 0.9894\n",
            "Epoch 903/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0277 - acc: 0.9957 - val_loss: 0.0416 - val_acc: 0.9939\n",
            "Epoch 904/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0366 - acc: 0.9947 - val_loss: 0.0313 - val_acc: 0.9946\n",
            "Epoch 905/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0405 - acc: 0.9942 - val_loss: 0.0173 - val_acc: 0.9981\n",
            "Epoch 906/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0887 - acc: 0.9880 - val_loss: 0.0383 - val_acc: 0.9953\n",
            "Epoch 907/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0695 - acc: 0.9905 - val_loss: 0.0185 - val_acc: 0.9982\n",
            "Epoch 908/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0474 - acc: 0.9936 - val_loss: 0.0647 - val_acc: 0.9915\n",
            "Epoch 909/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0445 - acc: 0.9950 - val_loss: 0.0144 - val_acc: 0.9988\n",
            "Epoch 910/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0645 - acc: 0.9911 - val_loss: 0.1757 - val_acc: 0.9790\n",
            "Epoch 911/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0816 - acc: 0.9881 - val_loss: 0.0376 - val_acc: 0.9950\n",
            "Epoch 912/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0263 - acc: 0.9957 - val_loss: 0.1284 - val_acc: 0.9836\n",
            "Epoch 913/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0482 - acc: 0.9931 - val_loss: 0.0555 - val_acc: 0.9873\n",
            "Epoch 914/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0526 - acc: 0.9922 - val_loss: 0.0129 - val_acc: 0.9984\n",
            "Epoch 915/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0645 - acc: 0.9902 - val_loss: 0.0434 - val_acc: 0.9925\n",
            "Epoch 916/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0284 - acc: 0.9956 - val_loss: 0.0233 - val_acc: 0.9967\n",
            "Epoch 917/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0324 - acc: 0.9949 - val_loss: 0.0271 - val_acc: 0.9967\n",
            "Epoch 918/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0537 - acc: 0.9916 - val_loss: 0.0439 - val_acc: 0.9930\n",
            "Epoch 919/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0530 - acc: 0.9915 - val_loss: 0.0148 - val_acc: 0.9982\n",
            "Epoch 920/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0581 - acc: 0.9929 - val_loss: 0.0245 - val_acc: 0.9972\n",
            "Epoch 921/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0350 - acc: 0.9938 - val_loss: 0.0125 - val_acc: 0.9985\n",
            "Epoch 922/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0345 - acc: 0.9948 - val_loss: 0.0671 - val_acc: 0.9898\n",
            "Epoch 923/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0489 - acc: 0.9931 - val_loss: 0.0396 - val_acc: 0.9947\n",
            "Epoch 924/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0545 - acc: 0.9920 - val_loss: 0.0257 - val_acc: 0.9969\n",
            "Epoch 925/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0497 - acc: 0.9920 - val_loss: 0.0129 - val_acc: 0.9987\n",
            "Epoch 926/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0309 - acc: 0.9953 - val_loss: 0.0244 - val_acc: 0.9972\n",
            "Epoch 927/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0409 - acc: 0.9941 - val_loss: 0.0526 - val_acc: 0.9926\n",
            "Epoch 928/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0736 - acc: 0.9877 - val_loss: 0.0155 - val_acc: 0.9991\n",
            "Epoch 929/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0346 - acc: 0.9940 - val_loss: 0.0158 - val_acc: 0.9977\n",
            "Epoch 930/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0932 - acc: 0.9899 - val_loss: 0.1214 - val_acc: 0.9832\n",
            "Epoch 931/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0357 - acc: 0.9944 - val_loss: 0.0711 - val_acc: 0.9741\n",
            "Epoch 932/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0410 - acc: 0.9934 - val_loss: 0.0860 - val_acc: 0.9899\n",
            "Epoch 933/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0242 - acc: 0.9966 - val_loss: 0.0084 - val_acc: 0.9991\n",
            "Epoch 934/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0702 - acc: 0.9918 - val_loss: 0.0173 - val_acc: 0.9980\n",
            "Epoch 935/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0284 - acc: 0.9956 - val_loss: 0.0135 - val_acc: 0.9979\n",
            "Epoch 936/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.0953 - acc: 0.9893 - val_loss: 0.0135 - val_acc: 0.9988\n",
            "Epoch 937/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0310 - acc: 0.9954 - val_loss: 0.2754 - val_acc: 0.9414\n",
            "Epoch 938/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0323 - acc: 0.9953 - val_loss: 0.0215 - val_acc: 0.9972\n",
            "Epoch 939/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0765 - acc: 0.9931 - val_loss: 0.0896 - val_acc: 0.9902\n",
            "Epoch 940/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0213 - acc: 0.9966 - val_loss: 0.0142 - val_acc: 0.9980\n",
            "Epoch 941/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0759 - acc: 0.9911 - val_loss: 0.0411 - val_acc: 0.9936\n",
            "Epoch 942/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0463 - acc: 0.9926 - val_loss: 0.0169 - val_acc: 0.9972\n",
            "Epoch 943/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0473 - acc: 0.9924 - val_loss: 0.0332 - val_acc: 0.9955\n",
            "Epoch 944/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.0229 - acc: 0.9963 - val_loss: 0.0249 - val_acc: 0.9940\n",
            "Epoch 945/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0528 - acc: 0.9930 - val_loss: 0.0988 - val_acc: 0.9879\n",
            "Epoch 946/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0683 - acc: 0.9894 - val_loss: 0.0445 - val_acc: 0.9940\n",
            "Epoch 947/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0456 - acc: 0.9942 - val_loss: 0.0111 - val_acc: 0.9991\n",
            "Epoch 948/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0335 - acc: 0.9943 - val_loss: 0.1567 - val_acc: 0.9830\n",
            "Epoch 949/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0398 - acc: 0.9947 - val_loss: 0.0833 - val_acc: 0.9833\n",
            "Epoch 950/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0504 - acc: 0.9928 - val_loss: 0.0192 - val_acc: 0.9975\n",
            "Epoch 951/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0473 - acc: 0.9933 - val_loss: 0.0258 - val_acc: 0.9968\n",
            "Epoch 952/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0353 - acc: 0.9949 - val_loss: 0.0130 - val_acc: 0.9985\n",
            "Epoch 953/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0582 - acc: 0.9913 - val_loss: 0.0295 - val_acc: 0.9954\n",
            "Epoch 954/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0471 - acc: 0.9934 - val_loss: 0.0199 - val_acc: 0.9977\n",
            "Epoch 955/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0226 - acc: 0.9966 - val_loss: 0.1104 - val_acc: 0.9864\n",
            "Epoch 956/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0501 - acc: 0.9937 - val_loss: 0.0308 - val_acc: 0.9959\n",
            "Epoch 957/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0404 - acc: 0.9948 - val_loss: 0.0244 - val_acc: 0.9961\n",
            "Epoch 958/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0358 - acc: 0.9956 - val_loss: 0.0355 - val_acc: 0.9950\n",
            "Epoch 959/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0499 - acc: 0.9933 - val_loss: 0.0244 - val_acc: 0.9961\n",
            "Epoch 960/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0903 - acc: 0.9880 - val_loss: 0.0222 - val_acc: 0.9971\n",
            "Epoch 961/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0610 - acc: 0.9922 - val_loss: 0.0203 - val_acc: 0.9969\n",
            "Epoch 962/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0312 - acc: 0.9957 - val_loss: 0.0110 - val_acc: 0.9990\n",
            "Epoch 963/1000\n",
            "54924/54924 [==============================] - 3s 57us/step - loss: 0.0875 - acc: 0.9908 - val_loss: 0.0140 - val_acc: 0.9981\n",
            "Epoch 964/1000\n",
            "54924/54924 [==============================] - 3s 53us/step - loss: 0.0537 - acc: 0.9916 - val_loss: 0.0207 - val_acc: 0.9973\n",
            "Epoch 965/1000\n",
            "54924/54924 [==============================] - 3s 54us/step - loss: 0.0371 - acc: 0.9939 - val_loss: 0.0144 - val_acc: 0.9985\n",
            "Epoch 966/1000\n",
            "54924/54924 [==============================] - 3s 52us/step - loss: 0.0582 - acc: 0.9904 - val_loss: 0.0443 - val_acc: 0.9934\n",
            "Epoch 967/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0699 - acc: 0.9894 - val_loss: 0.0223 - val_acc: 0.9983\n",
            "Epoch 968/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0498 - acc: 0.9922 - val_loss: 0.0626 - val_acc: 0.9910\n",
            "Epoch 969/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0267 - acc: 0.9959 - val_loss: 0.0129 - val_acc: 0.9988\n",
            "Epoch 970/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0275 - acc: 0.9956 - val_loss: 0.0313 - val_acc: 0.9957\n",
            "Epoch 971/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0608 - acc: 0.9915 - val_loss: 0.0418 - val_acc: 0.9929\n",
            "Epoch 972/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0434 - acc: 0.9927 - val_loss: 0.0565 - val_acc: 0.9917\n",
            "Epoch 973/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0426 - acc: 0.9933 - val_loss: 0.0136 - val_acc: 0.9988\n",
            "Epoch 974/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0562 - acc: 0.9926 - val_loss: 0.0112 - val_acc: 0.9984\n",
            "Epoch 975/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0521 - acc: 0.9918 - val_loss: 0.0348 - val_acc: 0.9949\n",
            "Epoch 976/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0361 - acc: 0.9943 - val_loss: 0.0126 - val_acc: 0.9981\n",
            "Epoch 977/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0321 - acc: 0.9951 - val_loss: 0.0169 - val_acc: 0.9973\n",
            "Epoch 978/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0361 - acc: 0.9940 - val_loss: 0.0248 - val_acc: 0.9964\n",
            "Epoch 979/1000\n",
            "54924/54924 [==============================] - 3s 51us/step - loss: 0.0373 - acc: 0.9942 - val_loss: 0.0183 - val_acc: 0.9969\n",
            "Epoch 980/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0509 - acc: 0.9912 - val_loss: 0.0733 - val_acc: 0.9897\n",
            "Epoch 981/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0284 - acc: 0.9957 - val_loss: 0.0257 - val_acc: 0.9971\n",
            "Epoch 982/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0179 - acc: 0.9973 - val_loss: 0.0319 - val_acc: 0.9956\n",
            "Epoch 983/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0368 - acc: 0.9940 - val_loss: 0.0318 - val_acc: 0.9951\n",
            "Epoch 984/1000\n",
            "54924/54924 [==============================] - 2s 42us/step - loss: 0.0497 - acc: 0.9932 - val_loss: 0.0135 - val_acc: 0.9992\n",
            "Epoch 985/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0422 - acc: 0.9932 - val_loss: 0.2789 - val_acc: 0.9743\n",
            "Epoch 986/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0426 - acc: 0.9946 - val_loss: 0.0176 - val_acc: 0.9975\n",
            "Epoch 987/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0444 - acc: 0.9939 - val_loss: 0.0109 - val_acc: 0.9994\n",
            "Epoch 988/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0440 - acc: 0.9926 - val_loss: 0.0109 - val_acc: 0.9993\n",
            "Epoch 989/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0359 - acc: 0.9943 - val_loss: 0.0274 - val_acc: 0.9960\n",
            "Epoch 990/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0571 - acc: 0.9918 - val_loss: 0.1021 - val_acc: 0.9859\n",
            "Epoch 991/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0506 - acc: 0.9919 - val_loss: 0.0129 - val_acc: 0.9995\n",
            "Epoch 992/1000\n",
            "54924/54924 [==============================] - 2s 44us/step - loss: 0.0240 - acc: 0.9958 - val_loss: 0.0143 - val_acc: 0.9985\n",
            "Epoch 993/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0494 - acc: 0.9918 - val_loss: 0.0223 - val_acc: 0.9956\n",
            "Epoch 994/1000\n",
            "54924/54924 [==============================] - 2s 43us/step - loss: 0.0226 - acc: 0.9962 - val_loss: 0.1847 - val_acc: 0.9513\n",
            "Epoch 995/1000\n",
            "54924/54924 [==============================] - 3s 46us/step - loss: 0.0347 - acc: 0.9945 - val_loss: 0.0233 - val_acc: 0.9962\n",
            "Epoch 996/1000\n",
            "54924/54924 [==============================] - 3s 49us/step - loss: 0.0562 - acc: 0.9906 - val_loss: 0.1080 - val_acc: 0.9841\n",
            "Epoch 997/1000\n",
            "54924/54924 [==============================] - 3s 48us/step - loss: 0.0447 - acc: 0.9913 - val_loss: 0.0213 - val_acc: 0.9971\n",
            "Epoch 998/1000\n",
            "54924/54924 [==============================] - 3s 47us/step - loss: 0.0246 - acc: 0.9957 - val_loss: 0.0297 - val_acc: 0.9960\n",
            "Epoch 999/1000\n",
            "54924/54924 [==============================] - 2s 45us/step - loss: 0.0556 - acc: 0.9919 - val_loss: 0.0457 - val_acc: 0.9914\n",
            "Epoch 1000/1000\n",
            "54924/54924 [==============================] - 3s 50us/step - loss: 0.0247 - acc: 0.9960 - val_loss: 0.0258 - val_acc: 0.9962\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6RFAgEdZvNJ",
        "colab_type": "code",
        "outputId": "edff3903-7cd1-4947-f248-7e5235adb12f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 760
        }
      },
      "source": [
        "Y_predict = model.predict(X_test)\n",
        "Y_predict =(Y_predict>0.5)\n",
        "print(confusion_matrix(Y_test, Y_predict))\n",
        "print(classification_report(Y_test, Y_predict))\n",
        "\n",
        "pyplot.plot(history.history['acc'])\n",
        "pyplot.plot(history.history['val_acc'])\n",
        "pyplot.title('Model Train vs Test Accuracy')\n",
        "pyplot.ylabel('accuracy')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'test'], loc='upper left')\n",
        "pyplot.show()\n",
        "\n",
        "pyplot.plot(history.history['loss'])\n",
        "pyplot.plot(history.history['val_loss'])\n",
        "pyplot.title('Model Train vs Test loss')\n",
        "pyplot.ylabel('loss')\n",
        "pyplot.xlabel('epoch')\n",
        "pyplot.legend(['train', 'test'], loc='upper left')\n",
        "pyplot.show()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[27982     0]\n",
            " [  105  1337]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       1.00      1.00      1.00     27982\n",
            "        True       1.00      0.93      0.96      1442\n",
            "\n",
            "    accuracy                           1.00     29424\n",
            "   macro avg       1.00      0.96      0.98     29424\n",
            "weighted avg       1.00      1.00      1.00     29424\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEWCAYAAACXGLsWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTSvbM3tC6xUAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRQ24X4pgO4X",
        "colab_type": "code",
        "outputId": "73997115-40d9-4269-e827-e4c45243168a",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 91
        }
      },
      "source": [
        "#Test a set of data not seen before, and with no classification to emulate user input\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "for fn in uploaded.keys():\n",
        "  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n",
        "      name=fn, length=len(uploaded[fn])))\n",
        "  \n",
        "import pandas as pd  \n",
        "userdata = pd.read_csv(\"TestUserDataFalse (5).csv\")\n",
        "\n",
        "userdata_X = userdata.values\n",
        "userdata_y = userdata.Defective.values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-049e76f2-c17a-44b8-8100-2b2ee3f0491f\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-049e76f2-c17a-44b8-8100-2b2ee3f0491f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving TestUserDataFalse.csv to TestUserDataFalse (5).csv\n",
            "User uploaded file \"TestUserDataFalse.csv\" with length 394 bytes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYicDFN7nyyY",
        "colab_type": "code",
        "outputId": "bc040d75-e192-4d4b-cd8f-d61258700f48",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "user_predict = model.predict(userdata_X)\n",
        "user_predict =(user_predict>0.5)\n",
        "print(confusion_matrix(userdata_y, user_predict))\n",
        "print(classification_report(userdata_y, user_predict))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       False       1.00      1.00      1.00         1\n",
            "\n",
            "    accuracy                           1.00         1\n",
            "   macro avg       1.00      1.00      1.00         1\n",
            "weighted avg       1.00      1.00      1.00         1\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}